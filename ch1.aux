\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Technical introduction}{13}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:main_concepts}{{1}{13}{Technical introduction}{chapter.1}{}}
\@writefile{toc}{\etoc@startlocaltoc{2}}
\citation{deng2013new,chan2016listen,abdel2013exploring}
\citation{fu2017look,traore2018deep,zheng2017learning}
\citation{collobert2008unified,goldberg2017neural}
\citation{cybenko1989approximation,hornik1991approximation,pinkus1999approximation,lu2017expressive,liang2016deep}
\citation{rumelhart1985learning,hecht1992theory}
\citation{ruder2016overview}
\citation{fausett2006fundamentals,medsker2001recurrent,mikolov2014learning}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Deep learning}{14}{section.1.1}\protected@file@percent }
\newlabel{sub:nn}{{1.1}{14}{Deep learning}{section.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Fundamental principle}{14}{subsection.1.1.1}\protected@file@percent }
\newlabel{sub:principle}{{1.1.1}{14}{Fundamental principle}{subsection.1.1.1}{}}
\@writefile{brf}{\backcite{deng2013new}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{chan2016listen}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{abdel2013exploring}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{fu2017look}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{traore2018deep}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{zheng2017learning}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{collobert2008unified}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{goldberg2017neural}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{cybenko1989approximation}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{hornik1991approximation}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{pinkus1999approximation}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{lu2017expressive}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{liang2016deep}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{rumelhart1985learning}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{hecht1992theory}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{brf}{\backcite{ruder2016overview}{{14}{1.1.1}{subsection.1.1.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Deep neural networks architectures for sequential data}{14}{subsection.1.1.2}\protected@file@percent }
\newlabel{subsec:neural_networks}{{1.1.2}{14}{Deep neural networks architectures for sequential data}{subsection.1.1.2}{}}
\@writefile{brf}{\backcite{fausett2006fundamentals}{{14}{1.1.2}{subsection.1.1.2}}}
\@writefile{brf}{\backcite{medsker2001recurrent}{{14}{1.1.2}{subsection.1.1.2}}}
\@writefile{brf}{\backcite{mikolov2014learning}{{15}{1.1.2}{subsection.1.1.2}}}
\newlabel{eq:rnn_v1}{{1.1}{15}{Deep neural networks architectures for sequential data}{equation.1.1.1}{}}
\newlabel{eq:rnn_v2}{{1.2}{15}{Deep neural networks architectures for sequential data}{equation.1.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Graphical representation of a Recurrent Neural Network. The recurrent connections between the nodes highlight the network's ability to process sequences of data by maintaining a `memory' of previous inputs through the hidden states.\relax }}{15}{figure.caption.8}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:rnn_intro}{{1.1}{15}{Graphical representation of a Recurrent Neural Network. The recurrent connections between the nodes highlight the network's ability to process sequences of data by maintaining a `memory' of previous inputs through the hidden states.\relax }{figure.caption.8}{}}
\citation{sherstinsky2020fundamentals,LSTM,GRU}
\citation{Hube67,White-MLE}
\citation{dempster1977maximum}
\citation{jaakkola2000bayesian,Blei_2017}
\@writefile{brf}{\backcite{sherstinsky2020fundamentals}{{16}{1.1.2}{remark.1.1.1}}}
\@writefile{brf}{\backcite{LSTM}{{16}{1.1.2}{remark.1.1.1}}}
\@writefile{brf}{\backcite{GRU}{{16}{1.1.2}{remark.1.1.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Bayesian estimation}{16}{section.1.2}\protected@file@percent }
\newlabel{sec:bayesian_estimation}{{1.2}{16}{Bayesian estimation}{section.1.2}{}}
\newlabel{eq:posterior}{{1.2}{16}{Bayesian estimation}{section.1.2}{}}
\citation{Blei_2017}
\@writefile{brf}{\backcite{Hube67}{{17}{1.2}{section.1.2}}}
\@writefile{brf}{\backcite{White-MLE}{{17}{1.2}{section.1.2}}}
\@writefile{brf}{\backcite{dempster1977maximum}{{17}{1.2}{section.1.2}}}
\@writefile{brf}{\backcite{jaakkola2000bayesian}{{17}{1.2}{section.1.2}}}
\@writefile{brf}{\backcite{Blei_2017}{{17}{1.2}{section.1.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Approximated Maximum Likelihood estimation with Variational Inference}{17}{subsection.1.2.1}\protected@file@percent }
\newlabel{subsec:vbi}{{1.2.1}{17}{Approximated Maximum Likelihood estimation with Variational Inference}{subsection.1.2.1}{}}
\@writefile{brf}{\backcite{Blei_2017}{{17}{1.2.1}{subsection.1.2.1}}}
\citation{bishop2006pattern}
\citation{variational-EM}
\citation{dempster1977maximum}
\citation{kingma2014}
\citation{maddison2016concrete,jang2016categorical}
\citation{kingma2014}
\newlabel{eq:DKL-2}{{1.3}{18}{Approximated Maximum Likelihood estimation with Variational Inference}{equation.1.2.3}{}}
\@writefile{brf}{\backcite{bishop2006pattern}{{18}{1.2.1}{equation.1.2.3}}}
\newlabel{eq:elbo}{{1.4}{18}{Approximated Maximum Likelihood estimation with Variational Inference}{equation.1.2.4}{}}
\@writefile{brf}{\backcite{variational-EM}{{18}{1.2.1}{equation.1.2.4}}}
\@writefile{brf}{\backcite{dempster1977maximum}{{18}{1.2.1}{equation.1.2.4}}}
\newlabel{subsec:optimization_vae}{{1.2.1}{18}{Approximated Maximum Likelihood estimation with Variational Inference}{equation.1.2.4}{}}
\@writefile{brf}{\backcite{kingma2014}{{18}{1.2.1}{equation.1.2.4}}}
\@writefile{brf}{\backcite{maddison2016concrete}{{18}{1.2.1}{equation.1.2.4}}}
\@writefile{brf}{\backcite{jang2016categorical}{{18}{1.2.1}{equation.1.2.4}}}
\@writefile{toc}{\contentsline {paragraph}{Continuous latent variables: }{18}{equation.1.2.4}\protected@file@percent }
\newlabel{subsec:reparameterization_trick}{{1.2.1}{18}{Continuous latent variables: }{equation.1.2.4}{}}
\@writefile{brf}{\backcite{kingma2014}{{18}{1.2.1}{equation.1.2.4}}}
\newlabel{eq:reparameterization_trick}{{1.5}{18}{Continuous latent variables: }{equation.1.2.5}{}}
\citation{kingma2014}
\citation{kingma2014}
\newlabel{eq:expectation_reparameterization}{{1.2.1}{19}{Continuous latent variables: }{equation.1.2.5}{}}
\newlabel{fig:rt_original}{{1.2a}{19}{Original form.\relax }{figure.caption.9}{}}
\newlabel{sub@fig:rt_original}{{a}{19}{Original form.\relax }{figure.caption.9}{}}
\newlabel{fig:rt}{{1.2b}{19}{Reparameterized form.\relax }{figure.caption.9}{}}
\newlabel{sub@fig:rt}{{b}{19}{Reparameterized form.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Illustration of the reparameterization trick. In the original form, we cannot compute the gradient of $f$ w.r.t $\phi $. While in the reparameterized form, gradient of $f$ w.r.t $\phi $ is easily computed. Diamonds indicate no stochasticity, while blue circles highlight its presence. Figure based on~\citep  {kingma2014}.\relax }}{19}{figure.caption.9}\protected@file@percent }
\@writefile{brf}{\backcite{kingma2014}{{19}{1.2}{figure.caption.9}}}
\newlabel{fig:rep_trick_cont}{{1.2}{19}{Illustration of the reparameterization trick. In the original form, we cannot compute the gradient of $f$ w.r.t $\phi $. While in the reparameterized form, gradient of $f$ w.r.t $\phi $ is easily computed. Diamonds indicate no stochasticity, while blue circles highlight its presence. Figure based on~\citep {kingma2014}.\relax }{figure.caption.9}{}}
\newlabel{ex:gaussian_case}{{1.2.1}{19}{}{example.1.2.1}{}}
\citation{gumbel1948statistical,maddison2014sampling}
\citation{maddison2016concrete,jang2016categorical}
\newlabel{eq:elbo_vae_mc}{{1.2.1}{20}{}{example.1.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Illustration of a Gaussian-Variational AutoEncoder model.\relax }}{20}{figure.caption.10}\protected@file@percent }
\newlabel{fig:rt_example_gaussian}{{1.3}{20}{Illustration of a Gaussian-Variational AutoEncoder model.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Discrete latent variables:}{20}{figure.caption.10}\protected@file@percent }
\@writefile{brf}{\backcite{gumbel1948statistical}{{20}{1.2.1}{figure.caption.10}}}
\@writefile{brf}{\backcite{maddison2014sampling}{{20}{1.2.1}{figure.caption.10}}}
\citation{maddison2016concrete}
\citation{maddison2016concrete}
\@writefile{brf}{\backcite{maddison2016concrete}{{21}{1.2.1}{figure.caption.10}}}
\@writefile{brf}{\backcite{jang2016categorical}{{21}{1.2.1}{figure.caption.10}}}
\newlabel{eq:gumbel_softmax}{{1.2.1}{21}{Discrete latent variables:}{figure.caption.10}{}}
\@writefile{brf}{\backcite{maddison2016concrete}{{21}{1.2.1}{figure.caption.10}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Illustration of the Gumbel-Max and Gumbel-Softmax tricks with $C=3$. The blue circle represents the Gumbel samples drawn from $\text  {Gumbel}(0,1)$. The result of the Gumbel-Max trick is the index $c$ of the maximum value and the result of the Gumbel-Softmax trick is a $C$-dimensional vector $z^{G-S}$ with values in $[0,1]^C$, which is a continuous, differentiable approximation of the $\argmax  $. \relax }}{21}{figure.caption.11}\protected@file@percent }
\newlabel{fig:rt_gumbel_summary}{{1.4}{21}{Illustration of the Gumbel-Max and Gumbel-Softmax tricks with $C=3$. The blue circle represents the Gumbel samples drawn from $\text {Gumbel}(0,1)$. The result of the Gumbel-Max trick is the index $c$ of the maximum value and the result of the Gumbel-Softmax trick is a $C$-dimensional vector $\latent ^{G-S}$ with values in $[0,1]^C$, which is a continuous, differentiable approximation of the $\argmax $. \relax }{figure.caption.11}{}}
\newlabel{rem:gumbel_softmax}{{1.2.1}{21}{}{remark.1.2.1}{}}
\citation{Blei_2017}
\citation{doucet2001introduction}
\@writefile{brf}{\backcite{maddison2016concrete}{{22}{1.2.1}{remark.1.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Posterior distribution}{22}{subsection.1.2.2}\protected@file@percent }
\newlabel{sub:posterior_distribution}{{1.2.2}{22}{Posterior distribution}{subsection.1.2.2}{}}
\@writefile{brf}{\backcite{Blei_2017}{{22}{1.2.2}{subsection.1.2.2}}}
\@writefile{brf}{\backcite{doucet2001introduction}{{22}{1.2.2}{subsection.1.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Discussion}{23}{subsection.1.2.3}\protected@file@percent }
\newlabel{sub:discussion}{{1.2.3}{23}{Discussion}{subsection.1.2.3}{}}
\newlabel{sub:unsupervised_bayesian_estimation_classification}{{1.2.3}{23}{}{subsection.1.2.3}{}}
\citation{douc2004asymptotic,Douc-ML-MIS}
\citation{livredoucetshort,chopin2020introduction}
\citation{kantas2015particle}
\citation{dempster1977maximum}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Sequential data modeling}{24}{section.1.3}\protected@file@percent }
\newlabel{sec:seq_data}{{1.3}{24}{Sequential data modeling}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Hidden Markov chains}{24}{subsection.1.3.1}\protected@file@percent }
\newlabel{sec:hmc}{{1.3.1}{24}{Hidden Markov chains}{subsection.1.3.1}{}}
\newlabel{eq:hmc_intro}{{1.6}{24}{Hidden Markov chains}{equation.1.3.6}{}}
\@writefile{brf}{\backcite{douc2004asymptotic}{{24}{1.3.1}{equation.1.3.6}}}
\@writefile{brf}{\backcite{Douc-ML-MIS}{{24}{1.3.1}{equation.1.3.6}}}
\citation{pieczynski2003pairwise,derrode2004signal,le2008fuzzy}
\citation{gorynin2018assessing}
\citation{wp-cras-chaines3,pieczynski2005triplet}
\citation{gorynin2018assessing,lanchantin2008unsupervised,pieczynski2007multisensor}
\@writefile{brf}{\backcite{livredoucetshort}{{25}{1.3.1}{equation.1.3.6}}}
\@writefile{brf}{\backcite{chopin2020introduction}{{25}{1.3.1}{equation.1.3.6}}}
\@writefile{brf}{\backcite{kantas2015particle}{{25}{1.3.1}{equation.1.3.6}}}
\@writefile{brf}{\backcite{dempster1977maximum}{{25}{1.3.1}{equation.1.3.6}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Pairwise Markov chains}{25}{subsection.1.3.2}\protected@file@percent }
\newlabel{sec:pairwise_triplet_mc}{{1.3.2}{25}{Pairwise Markov chains}{subsection.1.3.2}{}}
\@writefile{brf}{\backcite{pieczynski2003pairwise}{{25}{1.3.2}{subsection.1.3.2}}}
\@writefile{brf}{\backcite{derrode2004signal}{{25}{1.3.2}{subsection.1.3.2}}}
\@writefile{brf}{\backcite{le2008fuzzy}{{25}{1.3.2}{subsection.1.3.2}}}
\newlabel{eq:pmc_intro}{{1.7}{25}{Pairwise Markov chains}{equation.1.3.7}{}}
\@writefile{brf}{\backcite{gorynin2018assessing}{{25}{1.3.2}{equation.1.3.7}}}
\citation{lanchantin2004unsupervised}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Sequential generative models for Bayesian classification}{26}{subsection.1.3.3}\protected@file@percent }
\newlabel{sec:seq_gen_models}{{1.3.3}{26}{Sequential generative models for Bayesian classification}{subsection.1.3.3}{}}
\@writefile{brf}{\backcite{wp-cras-chaines3}{{26}{1.3.3}{subsection.1.3.3}}}
\@writefile{brf}{\backcite{pieczynski2005triplet}{{26}{1.3.3}{subsection.1.3.3}}}
\@writefile{brf}{\backcite{gorynin2018assessing}{{26}{1.3.3}{subsection.1.3.3}}}
\@writefile{brf}{\backcite{lanchantin2008unsupervised}{{26}{1.3.3}{subsection.1.3.3}}}
\@writefile{brf}{\backcite{pieczynski2007multisensor}{{26}{1.3.3}{subsection.1.3.3}}}
\newlabel{eq:tmc_intro}{{1.8}{26}{Sequential generative models for Bayesian classification}{equation.1.3.8}{}}
\@writefile{brf}{\backcite{lanchantin2004unsupervised}{{26}{1.3.3}{equation.1.3.8}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Organization of the thesis}{26}{subsection.1.3.4}\protected@file@percent }
\@setckpt{ch1}{
\setcounter{page}{29}
\setcounter{equation}{8}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{3}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{0}
\setcounter{NAT@ctr}{0}
\setcounter{nlinenum}{0}
\setcounter{etoc@tocid}{2}
\setcounter{blindtext}{1}
\setcounter{Blindtext}{5}
\setcounter{blind@countparstart}{0}
\setcounter{blindlist}{0}
\setcounter{blindlistlevel}{0}
\setcounter{blindlist@level}{0}
\setcounter{blind@listcount}{0}
\setcounter{blind@levelcount}{0}
\setcounter{blind@randomcount}{0}
\setcounter{blind@randommax}{0}
\setcounter{blind@pangramcount}{0}
\setcounter{blind@pangrammax}{0}
\setcounter{section@level}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{18}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{tcbbreakpart}{0}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{0}
\setcounter{FN@totalid}{1}
\setcounter{pp@a@FN@totalid}{1}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{AM@survey}{0}
\setcounter{float@type}{32}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{definition}{0}
\setcounter{example}{0}
\setcounter{remark}{0}
\setcounter{remarks}{0}
\setcounter{proposition}{0}
\setcounter{theorem}{0}
\setcounter{usecaseinner}{0}
\setcounter{lemma}{0}
\setcounter{model}{0}
\setcounter{corollary}{0}
\setcounter{scenario}{0}
\setcounter{su@anzahl}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstlisting}{0}
}
