% !TEx root = latex_avec_réduction_pour_impression_recto_verso_et_rognage_minimum.tex
\chapter{Generative hidden  Markov models}
\label{chap:pmc}

\localtableofcontents
\pagebreak

\section{Introduction}
This chapter introduces the \gls*{pmc} as a general
generative model that can be used to model
complex dependencies between the observations and the latent variables.
First, we recall  the 
\gls*{pmc} model~\citep{pieczynski2003pairwise}, 
and introduce how it can be used as a general model for generative modeling, 
from which the \gls*{hmc}~\citep{rabiner1989tutorial}, the \gls*{rnn}~\citep{medsker2001recurrent},
and the \gls*{gum}~\citep{salaun2019comparing} can be derived as particular cases.
% can be derived as particular cases.
Moreover, we present a general parameterization of the \gls*{pmc} model, 
which includes a deep parameterization (DNNs).
In the second part, we develop an adapted VI
 algorithm~\citep{jaakkola2000bayesian,Blei_2017}
for a general parameter estimation of this model, 
which can be applied to any \gls*{pmc} model, linear or not, Gaussian or not.
We provide some experimental results, 
demonstrate \gls*{pmc} as a generative model, 
and see how it compares to other popular models 
that use latent variables and DNNs.
To conclude, we  focus on a particular instance of the \gls*{pmc} model,
the linear, and stationary Gaussian \gls*{pmc}, 
for a theoretical analysis of the generative power of the \gls*{pmc} model.
This analysis is based on the expressivity 
of the \gls*{pmc}, 
\ie~the distribution of the observations generated by the model.


% \section{General Pairwise Markov Chain}
% \subsection{Model definition}

\section{The pairwise Markov chain as a unified model}
\label{sec:pmc_unified_model}

% \yohan{Maybe put 2/3 lines to introduce the chapter : here we first cast popular
% generative models into a general PMC model. Next we propose a parameter
% estimation method based on sequential VI\@. We compare our generalization with
% current generative models. Finally, we try to quantify the interest of such
% models.}
We recall the notation introduced in Chapter~\ref{chap:main_concepts},
$\obs_{0:T}=(\obs_{0}, \dots,\; \obs_{T})$, $\obs_t \in \mathbb{R}^{d_{\obs}}$, and
$\latent_{0:T}=(\latent_{0}, \dots,\; \latent_{T})$,  $\latent_t \in \mathbb{R}^{d_{\latent}}$
which are two sequences of observed and latent  random variables (r.v.), respectively, 
of length $T+1$.
This chapter focuses on the case where the latent variables 
are not interpretable as the labels of the observations. In other words,
our interest is a generative model, where the latent variables are 
just an intermediate step to create a complex distribution of the observations.
For that, we recall the joint distribution of the observed and latent variables
in the PMC model~\eqref{eq:pmc_intro},  which is given by
\begin{equation*}
    % \label{eq:pmc_general}
    \p(\latent_{0:T}, \obs_{0:T}) = \p(\latent_0,\obs_0)\prod_{t=1}^T 
    \p(\latent_t,\obs_t|\latent_{t-1},\obs_{t-1}) \text{, } 
    \quad  \text{for all } T \in \NN  \text{.}
\end{equation*}
% % which is a direct generalization of \gls*{hmc} where 
% where we %only assume 
% assume that the pair $(\latent_T,\obs_T)$ is
% Markovian~\citep{pieczynski2003pairwise, le2008fuzzy, morales2021variational}.
% In such a model, $\latent_T$ is not necessarily a Markov chain,
% and the observations can now be dependent given $\latent_T$.

From a modeling point of view, the choice of the transition distribution
$\p(\latent_t,\obs_t|\latent_{t-1},\obs_{t-1})$ is a thorny problem.
The transition distribution can be factorized
in different ways. The choice of the factorization depends on the specific problem
and the underlying  assumptions  about the dependencies between variables.
In practice, we need to choose a transition distribution that has an impact on
the relevance of the model $\p(\obs_{0:T})$ and is able to fit the data.
To that end, we consider the following factorization for the transition distribution:
\begin{align}
    \label{eq:pmc_gen_transition}
    \p(\latent_t,\obs_t|& \latent_{t-1},\obs_{t-1}) =
    \p(\latent_t|\latent_{t-1},\obs_{t-1})\p(\obs_t|\latent_{t-1:t},\obs_{t-1}) \text{.}
    % % \label{eq:pmc_gen2}
    % \p(\latent_t,\obs_t|& \latent_{t-1},\obs_{t-1}) =
    % \p(\latent_t|\latent_{t-1},\obs_{t-1:t})\p(\obs_t|\latent_{t-1},\obs_{t-1})\text{.}
\end{align}
The joint distribution of $(\obs_{0:T}, \latent_{0:T})$ reads
\begin{equation}
    \label{eq:pmc_gen_t}
    \p(\latent_{0:T}, \obs_{0:T}) =
    % \overset{\rm PMC}{=} 
    \p(\latent_0,\obs_0)\!\prod_{t=1}^T 
    \p(\latent_t|\latent_{t-1},\obs_{t-1})\p(\obs_t|\latent_{t-1:t},\obs_{t-1}) \text{, } \text{for all } T \!\in\! \NN  \text{.}
\end{equation}
This factorization assumes that the observation $\obs_t$ depends 
on the previous latent variables
$\latent_{t-1}$ and $\latent_t$, 
and not only on the current latent variable $\latent_t$
and the previous observation $\obs_{t-1}$.
While the distribution of latent variable $\latent_t$ is determined 
by the previous observation $\obs_{t-1}$ 
and the previous latent variable $\latent_{t-1}$.
The choice of this factorization is motivated by the fact that some popular
generative models based on latent variables can be derived from it,
such as the \gls*{hmc}, the \gls*{rnn}  and the \gls*{gum}~\citep{salaun2019comparing}.


% This unified  PMC is presented in the Section~\ref{sec:pmc_unified_model}.

% In \citep{salaun2019comparing}, the GUM is proposed as a unified framework
% to compare the expressivity of generative models based on latent variables,
% where the HMC and the RNN are considered
% as particular cases of the GUM and compared to the GUM.
Our objective is to cast the HMC, RNN and GUM generative models into
a more general one, the \gls*{pmc}. 
To this end, we recall the \gls*{hmc}~\eqref{eq:hmc_intro}
where the joint distribution of the observed and latent variables 
reads
\begin{equation*}
    \p(\obs_{0:T},\latent_{0:T}) \overset {\rm HMC}{=} 
    \p(\latent_0, \obs_0)\prod_{t=1}^T \p(\latent_t|\latent_{t-1}) 
    \p(\obs_t|\latent_{t}) \text{, } \; \text{for all } T \in \NN \text{.}
\end{equation*}
Here $(\obs_t, \latent_t)$  becomes conditionally independent of $\obs_{t-1}$ given $\latent_{t-1}$, and 
$\obs_t$, in addition, does not depend on $\latent_{t-1}$. %(Figure~\ref{fig:hmm}). 
% This equation describes the joint probability distribution of the
% observed variables and hidden variables in the HMC model. It
% involves the initial hidden state distribution $\p(\latent_0)$,
% the transition probabilities from $\latent_{t-1}$  to $\latent_t$  $(\p(\latent_t|\latent_{t-1}))$,
% and the observation probabilities $\p(\obs_t | \latent_t)$.
Similarly, the \gls*{gum} is a particular case of  
%~\citep{salaun2019comparing}   
the \gls*{pmc} %(Figure~\ref{fig:gum}) 
defined as follows
% \begin{align*}
%     \p(\latent_t,\obs_t|\latent_{t-1},\obs_{t-1}) =  \p(\latent_t|\latent_{t-1},\obs_{t-1})\p(\obs_t|\latent_{t}) \text{.}
% \end{align*}
% The joint distribution of the observed and latent variables in the \gls*{gum} is given by
\begin{equation*}
    \p(\obs_{0:T},\latent_{0:T})  \overset{\rm GUM}{=} \p(\latent_0, \obs_0)
    \prod_{t=1}^T \p(\latent_t|\latent_{t-1},\obs_{t-1})\p(\obs_t|\latent_{t}) 
    \text{, }  \text{for all } T \in \NN  \text{,}
\end{equation*}
where $\obs_t$ becomes conditionally
 independent of $(\obs_{t-1},\latent_{t-1})$. 
In the case of predicting future observations with RNNs,
a probabilistic approach seems more appropriate when we want to quantify 
the uncertainty associated with our prediction. To do this, we simply replace $\g$
by a parametric distribution $\p$  and $o_t$ by $\obs_{t+1}$
in Equation~\eqref{eq:rnn_v2}.
In addition, we use the transformation  $\latent_{t} \leftarrow \lat_{t-1}$   in 
equations~\eqref{eq:rnn_v1}-\eqref{eq:rnn_v2}
to obtain the following model:
\begin{align*}
    &\p(\obs_{0:T})\overset {\rm RNN}{=}  \p(\obs_{0}) \prod_{t=1}^T 
    \p(\obs_t|\latent_t	) \text{, } \text{for all } T \in \NN \text{,}\\
    &\p(\latent_t|\latent_{t-1},\obs_{t-1})\overset {\rm RNN}{=} 
    \delta_{\f(\latent_{t-1},\obs_{t-1})}(\latent_t) \text{, } \;  \latent_0\overset {\rm RNN}{=} 0 \text{, } \; \p(\obs_{0}|\latent_{0}) \overset {\rm RNN}{=}  \p(\obs_{0}) \text{.}
\end{align*}
Contrary to the \gls*{hmc} and the GUM, the \gls*{rnn} follows a different approach.
In the \gls*{rnn}, the latent variable $\latent_t$ is deterministically determined,
given the previous observation $\obs_{t-1}$ and the previous latent variable $\latent_{t-1}$.
With a slight abuse of notation,  $\p(\latent_t|\latent_{t-1},\obs_{t-1})$ 
coincides with the Dirac measure and is not a probability density function.
%  $\delta_{\f(\latent_{t-1},\obs_{t-1})}(\latent_t)$
% where $\f$ is a deterministic function.
The expression of $\latent_t$ relies on an activation function $\f$.
Similar to the HMC and the GUM, in the RNN $\obs_t$ depends on $\latent_t$ given the past.
We can see a common underlying framework that captures the joint probability
distributions of the observed and latent variables in these models.
The graphical structures of the  models are summarized in 
Figure~\ref{fig:graphical_models}.



\begin{figure}[htb]
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=6cm]{Figures/Graphical_models/hmc.pdf}
    \caption{HMC}
    \label{fig:hmm}
    \vspace{1.1cm}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.47\linewidth}
    \centering
    \includegraphics[width=6cm]{Figures/Graphical_models/rnn.pdf}
    \caption{RNN}
    \label{fig:rnn}
    \vspace{1.1cm}
  \end{subfigure}

  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=6.0cm]{Figures/Graphical_models/gum.pdf}
    \caption{GUM}
    \label{fig:gum}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=6.0cm]{Figures/Graphical_models/pmc.pdf}
    \caption{PMC}
    \label{fig:pmc}
  \end{subfigure}

  \caption{Conditional dependencies of the \gls*{hmc}, \gls*{rnn}, 
  \gls*{gum}, and \gls*{pmc}. In the \gls*{rnn}, 
  the hidden states $\latent_t$ are shown as diamonds to stress 
  that they are no source of stochasticity. 
  The \gls*{hmc}, \gls*{rnn}, and \gls*{gum} are particular cases of the \gls*{pmc}.}
  \label{fig:graphical_models}
\end{figure}



\begin{remark}
    \cite{salaun2019comparing} have proposed the GUM as a unified framework
    to compare the expressivity of generative models based on latent variables.
    The GUM can be seen as a stochastic version of the \gls*{rnn} which 
    includes popular generative models such as
    the Variational RNN~\citep{chung2015recurrent} and 
    the Stochastic RNN~\citep{fraccaro2016sequential} as particular cases, 
    with a latent variable  
    $\latent_t \leftarrow (\lat_{t},\latent_{t})$. 
\end{remark}


\section{Parameter estimation for general PMCs}
\label{sub:pmc_parameter_estimation}
In this section, we propose a VI approach to  estimate the parameters $\theta$ 
of general PMC models.
This new approach can be applied to any sequence $\obs_{0:T}$ of varying length $T$
and  does not require the knowledge of the latent variables $\latent_{0:T}$.
In addition, it is suitable for high dimensional models \citep{Blei_2017}.
First, we introduce a general parameterization of the PMC model
which includes a deep parameterization (via DNNs).
Next, we adapt the  (static) VI framework described in 
Subsection~\ref{subsec:vbi}  
for the sequential case with  PMCs.



% % \subsection{Monte Carlo Approximation}
% % Subsection~\ref{subsec:reparameterization_trick}
% % \eqref{eq:reparameterization_trick}


% % However, the samples depend on $\phi$; in order to ensure
% % that the Monte Carlo approximation remains differentiable w.r.t. $\phi$, a
% % sample $\latent_{0:T}^{(m)}$, for $m \in [1:M]$ where $M$ is the number of
% % samples. This sample has to be reparameterized as a differentiable function of
% % $\phi$ (reparameterization trick, see 
% % Section~\ref{subsec:reparameterization_trick}).\\ 
% % More precisely, a final sample $\latent_{0:
% % T}^{(m)}$ is obtained by
% % \begin{align}
% %     \label{eq:reparametrization_vpmc}
% %     \latent_{0:T}^{(m)} =& g(\phi,\epsilon_{0:T}^{(m)}) \text{,}
% % \end{align}
% % where $\epsilon^{(m)}$ is a random variable sampled from a distribution
% % which does not depend on $\phi$, and where $g$ is a differentiable function of $\phi$.\\
% Thus, the final approximation reads
% \begin{align}
%     \label{eq:elbo-gpmc-mc}
%     \hat{\Qgen}(\theta,\phi) &= -\frac{1}{M} \sum_{m=1}^M 
% \log \left(\frac{\q(\latent_{0:T}^{(m)}|\obs_{0:T})}{\p(\latent_{0:T}^{(m)},\obs_{0:T})}\right) \text{, } 
% \end{align}
% with $\latent_{t}^{(m)} \sim \q(\latent_{t}^{(m)}|\latent_{0:t-1}^{(m)},\obs_{0:T}) \text{, for }
%  m \!\in \![1\!:\!M] \text{.} $
% In addition, $\latent_{t}^{(m)}$ is a differentiable function of $\phi$, for  
% $m \in [1:M]$ and $t \in [0:T]$.
% Algorithm~\ref{algo:algo_train_dpmc_gen} summarizes the general estimation algorithm
% for  PMC models.
% % \begin{remark}
% %     In the case of DPMCs, their
% %     gradients are computable from the backpropagation
% %     algorithm~\citep{rumelhart1986learning} since $\pz$ and $\px$ 
% %     are differentiable w.r.t. $\theta$. 
% %     Moreover, the parameters $\phi$ of the variational distribution $\q$
% %     are the parameters of the DNNs which are differentiable w.r.t. $\phi$.
% %     Thus, Algorithm~\ref{algo:algo_train_dpmc_gen} can be applied to DPMCs.
% % \end{remark}



%  \begin{remark}
%     \label{rem:adam_pmc}
%     In the practical implementation of our algorithm~\ref{algo:algo_train_dpmc_gen} 
%     in Line~\eqref{eq:elbo_grad_vpmc},
%     we use the Adam optimizer. This choice is based on the optimizer's proven
%     effectiveness and efficiency in various machine learning 
%     applications~\citep{kingma2014adam}.
% \end{remark}
    
% \begin{example}
%     \label{example:gauss_variational}
% We consider the case of a  Gaussian variational distribution which
% satisfies 
% \begin{align*}
% \q(\latent_{t}|\latent_{0:t-1},\obs_{0:T})&=
% \q(\latent_{t}|\latent_{t-1},\obs_{t})\\
% &= \mathcal{N}\left(\latent_t; \mulatent , \diag(\siglatent) \right) 
% % \text{, where } [\mulatent , \siglatent] =  \qz(\latent_{t-1},\obs_{t}) \text{, } 
% \end{align*}
% where $ [\mulatent , \siglatent] =  \qz(\latent_{t-1},\obs_{t})$
% and $\qz$ is a differentiable vector function parameterized by $\phi$,
% which is assumed to be differentiable w.r.t. $\phi$; and $\diag(\;\cdot \;)$ denotes
% the diagonal matrix deduced from $\siglatent$. 
% For example, $\mulatent$ and $\siglatent$ represent the output of a neural network
% similar to the one presented in Figure~\ref{fig:dpmc_gaussian}.
% % ~\ref{ex:dpmc_gaussian}.


% In this case, a sample $\latent_{t}^{(m)}$, for all $i \in [1:M]$ and $t \in [0:T]$,
% can be reparameterized by using the reparameterization trick 
% % (see Section~\ref{subsec:reparameterization_trick}) 
% and  reads
% \begin{equation*}
% \latent_{t}^{(m)}=
% \mulatent(\latent_{t-1}^{(m)},\obs_t)+
% \diag(\siglatent(\latent_{t-1},\obs_t))^{\frac{1}{2}} 
% \times  \epsilon_t^{(m)} \text{,} \quad \quad \epsilon_t^{(m)} 
% \overset{i.i.d.}{\sim} \mathcal{N}(0,I) \text{.}
% \end{equation*}

% The  Monte Carlo approximation of the ELBO can be rewritten as
% $$\hat{\Qgen}(\theta,\phi)= - \frac{1}{M} \sum_{m=1}^M \log \left( \frac{\q(\latent_0^{(m)}|\obs_0)}{\p(\latent_0^{(m)},\obs_0)}\right) - 
% \frac{1}{M} \sum_{m=1}^M \sum_{t=1}^T \log \left(\frac{\q(\latent_t^{(m)}|\latent_{t-1}^{(m)},\obs_t)}{\p(\latent_t^{(m)},\obs_t|\latent_{t-1}^{(m)},\obs_{t-1})} \right)  $$
% and is next optimized w.r.t. $(\theta,\phi)$.\\
% \end{example}

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=0.5\textwidth]{Figures/Graphical_models/rep_trick_example.pdf}
%     \caption{Example reparameterization trick.}
%     \katy{Change this example}
%     \label{fig:rt_example}
% \end{figure}





\subsection{General parameterization of PMCs}
\label{sec:pmc_parameterization}
We propose a general parameterization of the PMC model that can be applied to
any PMC. Without loss of generality, we consider the transition distribution 
$\p(\latent_t,\obs_t|\latent_{t-1},\obs_{t-1})$
given in~\eqref{eq:pmc_gen_transition}.
A general parameterization allows us to consider different any (conditional) distributions
$\p(\latent_t|\latent_{t-1},\obs_{t-1})$ and $\p(\obs_t|\latent_{t-1:t},\obs_{t-1})$,
\eg~Gaussian distributions.
Thus, for fixed  distributions, the parameters are learned based on functions 
of the conditional variables. 
This parameterization  extends beyond linear functions and also includes the application of 
deep neural networks due to the universal approximation property
(see Section~\ref{sub:nn}).

Let~$\pz(\latent_{t-1}, \;\obs_{t-1})$ and~$\px(\latent_{t-1:t},\;\obs_{t-1} )$  
be two vector-valued functions 
of ~$(\latent_{t-1},\obs_{t-1})$ and of~$(\latent_{t-1:t},\obs_{t-1})$, respectively, 
that are assumed to be differentiable w.r.t. $\theta$. 
Let also $\eta(\latent; \param )$ and $\zeta(\obs;\param')$
be probability density functions (pdf) on $\mathbb{R}^{d_\latent}$ 
and $\mathbb{R}^{d_\obs}$, respectively,
whose parameters are given by the vectors $\param$ and $\param'$, respectively.
$\eta$ and $\zeta$ are assumed to be differentiable w.r.t. $\param$ and $\param'$, 
respectively.
Then, we parameterize the conditional distributions in~\eqref{eq:pmc_gen_t} 
as 
\begin{eqnarray}
% \label{pmc-transition}
% \p(\latent_t, \obs_t | \latent_{t-1}, \obs_{t-1}) = \p(\latent_t |\latent_{t-1},\obs_{t-1}) \p(\obs_t | \latent_{t-1:t},\obs_{t-1}) \text{,}\nonumber\\
\label{pmc-theta-1-gen}
\p(\latent_t|\latent_{t-1},\obs_{t-1})=\eta(\latent_t; \;  \pz(\latent_{t-1},\obs_{t-1}) )  \text{,} \\
\label{pmc-theta-2-gen}
\p(\obs_t|\latent_{t-1:t},\obs_{t-1})=\zeta(\obs_t;\; \px(\latent_{t-1:t},\obs_{t-1} ) ) \text{.}
\end{eqnarray}
In other words, $\pz$ (resp. $\px$)
describes the parameters of the (conditional) distribution $\eta$ 
(resp. $\zeta$).

\begin{example}
    \label{ex:gaussian}
    As an illustration, we consider $\eta$ as a multivariate  Gaussian distribution. 
    $\pz$  is the vector which contains the mean and the covariance matrix of $\eta$. 
    In this case, $\p(\latent_t|\latent_{t-1},\obs_{t-1})$ reads 
    \begin{align*}
        \p(\latent_t|\latent_{t-1},\obs_{t-1})  &= 
        \N\left(\latent_t;  \mulatentp , \Siglatentp \right)   \text{, where }   
        \left[ \mulatentp , \Siglatentp \right]= 
        \pz(\latent_{t-1},\obs_{t-1}) \text{,} 
    \end{align*}
    It shows how the mean and covariance matrix of this Gaussian distribution are derived
    from the values given by the function $\pz$, 
    which is assumed to be differentiable w.r.t. $\theta$.
% \katyobs{Pendiente hacer despues de ver el chap 4 para ver los ejemplos}
\end{example}

% \begin{remark} It is easy to verify  that from \eqref{eq:pmc_gen_t}, 
%     we deduce  the HMC \eqref{eq:hmc}
%     \begin{align}
%     p(\latent_t,\obs_t|& \latent_{t-1},\obs_{t-1}) =  %p(\latent_t|\latent_{t-1},\obs_{t-1})p(\obs_t|\latent_{t})
%     p(\latent_t|\latent_{t-1},\cancel{\obs_{t-1}})p(\obs_t|\latent_{t}\cancel{\obs_{t-1}}, \cancel{\latent_{t-1}}) \text{.}
%     \end{align}       
% \end{remark}

% \subsection{Deep Pairwise Markov Chain}
\paragraph{Deep pairwise Markov chain - }
\label{sec:dpmc}
A particular case of the general parameterization of the PMC model is the \gls*{dpmc},
where the parameterization of the transition distribution~$\p(\latent_t,\obs_t|\latent_{t-1},\obs_{t-1})$
presented in \eqref{pmc-theta-1-gen} and \eqref{pmc-theta-2-gen} is given by DNNs.
%  the distributions $\p(\latent_t | \latent_{t-1}, \obs_{t-1})$
% and  $\p(\obs_t | \latent_{t-1:t}, \obs_{t-1})$ 
Since DNNs can theoretically approximate any function which satisfies 
reasonable assumptions (see Section~\ref{subsec:neural_networks}),
% ~\citep{pinkus1999approximation}, 
our objective is to use them to
approximate any parameterization of $\eta$ and $\zeta$
of the distributions $\p(\latent_t | \latent_{t-1}, \obs_{t-1})$
and  $\p(\obs_t | \latent_{t-1:t}, \obs_{t-1})$, respectively.
In other words,  $\pz$ and $\px$ are the outputs of two \gls*{dnns}. 
For example, with $(\latent_{t-1},\obs_{t-1})$
and $(\latent_{t-1:t},\obs_{t-1})$ as inputs, respectively in~\eqref{pmc-theta-1-gen} and \eqref{pmc-theta-2-gen}.
The set of parameters $\theta$ now consists of the parameters of these 
\gls*{dnns}  (weights and biases). 
In this case, their gradients are computable from the backpropagation
algorithm~\citep{rumelhart1985learning,hecht1992theory} since $\pz$ and $\px$
are differentiable w.r.t. $\theta$ (see Section~\ref{sub:principle}).


% Note that a unique DNN is 
% used for $\pz$ (resp. $\px$) overtime. 

\begin{example}
    \label{ex:dpmc_gaussian}
    % \katyobs{check this example after the previous one}

    We recall the previous example~\ref{ex:gaussian}, where $\eta$ is a Gaussian distribution,
    In this case, the mean $\mulatentp$  and the covariance matrix 
    $\diag(\siglatentp)$ are the
    output of a neural network as illustrated in Figure~\ref{fig:dpmc_gaussian}.
    For example, $\mulatentp$ and $\siglatentp$ can be the output of a linear layer. 

    \begin{align*}
        \p(\latent_t|\latent_{t-1},\obs_{t-1})  &= \N\left(\latent_t;  \mulatentp , \diag(\siglatentp) \right)   \text{, where }   \left[ \mulatentp , \siglatentp \right]= \pz(\latent_{t-1},\obs_{t-1}) \text{.} 
    \end{align*}
    
    \begin{figure}[htb]
        \centering
        \includegraphics[width=0.8\textwidth]{Figures/Graphical_models/pmc_example.pdf}
        \caption{Illustration of a deep parameterization of the
        distribution~$\p(\latent_t|\latent_{t-1},\obs_{t-1})$, where the
        parameters~$\mulatentp$ and~$\siglatentp$ of the Gaussian distribution
        are the output of a DNN.}
        \label{fig:dpmc_gaussian}
    \end{figure}
    % \katyobs{Add an example of the activation function used for both the variance and the mean}
\end{example}

\subsection{Variational Inference for PMCs}
For PMCs, we can extend the ELBO given in~\eqref{eq:elbo} ,
which was formulated for static models to the sequential case.
We now define $\obs
\leftarrow \obs_{0:T}$, and $\latent \leftarrow \latent_{0:T}$.
Then the following inequality holds for any variational distribution 
$\q(\latent_{0:T}|\obs_{0:T})$,
\begin{align}
    \label{eq:elbo_general}
    \log(\p(\obs_{0:T})) & \geq - \int  \q(\latent_{0:T}|\obs_{0:T})
    \log  \left(\frac{\q(\latent_{0:T}|\obs_{0:T})}{\p(\obs_{0:T},\latent_{0:T})}\right) 
     {\rm d} \latent_{0:T}  = \Qgen(\theta,\phi) \text{,}
\end{align}
where  $\q$ depends on a set of parameters $\phi$.
In our sequential case, we choose the following variational distribution
\begin{align}
    \label{eq:var_dist}
    \q(\latent_{0:T}|\obs_{0:T})=& \q(\latent_{0}|\obs_{0:T}) 
    \prod_{t=1}^T \q(\latent_{t}|\latent_{0:t-1},\obs_{0:T}) \text{.} 
\end{align}
This general factorization, that is based on transitions 
$\q(\latent_{t}|\latent_{0:t-1},\obs_{0:T})$, captures 
the temporal dependencies inherent in sequential data.
This form involves a choice of a variational distribution
$\q(\latent_{t}|\latent_{0:t-1},\obs_{0:T})$,
and the parameters that govern this distribution remain constant across
time. % involves a choice of a variational distribution,so a  time-independent parameterization
The variational distribution $\q$ should respect the differentiability 
and computational tractability constraints.
For efficient optimization, $\q(\latent_{t}|\latent_{0:t-1},\obs_{0:T})$ should
be differentiable w.r.t. $\phi$ and 
should be chosen in a way that $\Qgen(\theta,\phi)$ is computable or can be
approximated (see Subsection~\ref{subsec:vbi}).
% This property is crucial for using
% gradient-based optimization algorithms, such as stochastic gradient descent, to
% learn the optimal parameters of the variational distribution $\q$.
% In addition, $\q$ should be chosen in a way that $\Qgen(\theta,\phi)$ is computable 
% or can be approximated (see Subsection~\ref{subsec:vbi}).
% % \katyobs{Check this paragraph}
Thus, the factorization of $\p(\latent_{0:T},\obs_{0:T})$ and 
$\q(\latent_{0:T}|\obs_{0:T})$  given by~\eqref{eq:pmc_gen_t}
and~\eqref{eq:var_dist}, respectively, allows us to rewrite the 
ELBO~\eqref{eq:elbo_general} as follows
\begin{align}
    \label{eq:elbo_vpmc}
    \Qgen(\theta,\phi) =& \L_1(\theta,\phi) +  \L_2(\theta,\phi)
\end{align}
where
\begin{align}
    \L_1(\theta,\phi) =& \; \E_{\q(\latent_0| \obs_{0:T} )} (  \log \p(\obs_0| \latent_0)) 
    \nonumber
    \\
    \label{eq:elbo_vpmc_l1}
    &+  
    \sum_{t=1}^T \E_{\q(\latent_t|\latent_{0:t-1},\obs_{0:T})}
    (  \log \p(\obs_t|\latent_{t-1:t},\obs_{t-1} ) )\text{,}\\
    \L_2(\theta,\phi) =& - \dkl(\q(\latent_0| \obs_{0:T} ) | |  \p(\latent_0))
    \nonumber
    \\
    \label{eq:elbo_vpmc_l2}
    & - \sum_{t=1}^T  \dkl(\q(\latent_t|\latent_{0:t-1},\obs_{0:T}) | | \p(\latent_t|\latent_{t-1},\obs_{t-1})) \text{.}
\end{align}
$\Qgen(\theta,\phi)$ involves the sum of two terms. The first term $\L_1(\theta,\phi)$ represents
a reconstruction term which measures the ability to reconstruct observations
according to the conditional likelihood $\p$ from the latent variables
distributed according to $\q$.
The second term $\L_2(\theta,\phi)$ involves a KLD term between 
the variational  $\q$ and the conditional prior $\p$ distributions,
which encourages $\q$ to be close to $\p$~\citep{kingma2014}.
% \subsection{Monte Carlo Approximation}
It remains to compute and optimize the 
ELBO~\eqref{eq:elbo_vpmc} w.r.t.~$(\theta,\phi)$ 
in order to estimate the parameters of the PMC model.
On one hand, the term $\L_2(\theta,\phi)$~\eqref{eq:elbo_vpmc_l2} 
involves the KLD between
$\q$ and $\p$.
% , which can often be integrated analytically~\citep{kingma2014}
% since $\q$ and $\p$ are assumed to be tractable distributions (\eg Gaussian
% distributions). 





\begin{algorithm}[htbp!]
    \caption{General parameter estimation for generative PMCs}
    \label{algo:algo_train_dpmc_gen}
  \begin{algorithmic}[1]
    \Require{$\obs_{0:T}$, the data; $\varrho$, the learning rate; $M$ the number of samples}
    \Ensure{$(\theta^*, \phi^*)$, sets of estimated parameters}
    \State Initialize the parameters $\theta^0$ and $\phi^0$
    \State $j\leftarrow 0$\label{line:start_vpmc}
    \While{\text{convergence is not attained}}
      \State Sample $\latent_0^{(m)}\sim q_{\phi^{{j}}}(\latent_0|\obs_{0:T})$,  for all  $1 \leq m \leq M$ 
      \State Sample $\latent_t^{(m)}\sim q_{\phi^{{j}}}(\latent_t|\latent_{0:t-1}^{(m)},\obs_{0:T})$,   for all  $1 \leq m \leq M$, for all $1 \leq t \leq T$ 
      \State Evaluate the loss $\widehat{\Qgen}(\theta^{{j}},\phi^{{j}})$ 
      from \eqref{eq:elbo_vpmc},
      ~\eqref{eq:elbo_vpmc_l2},
      and~\eqref{eq:elbo-pmc-mc-1}. 
      \label{line:evaluate_loss}
      \State{Compute the derivative of the loss function 
      $\nabla_{(\theta, \phi)} \widehat{\Qgen}(\theta,\phi)$.  
    %   from \eqref{eq:elbo-gpmc-mc}.
    }\label{line:derivate_pmc} 
      \State Update the parameters with gradient ascent
    \begin{equation}
    \begin{pmatrix}\theta^{(j+1)}\\\phi^{(j+1)}\end{pmatrix}=
    \begin{pmatrix}\theta^{{j}}\\\phi^{{j}}\end{pmatrix}
    + \varrho {\nabla_{(\theta, \phi)} \widehat{\Qgen}(\theta,\phi)}\Big|_{(\theta^{{j}},\phi^{{j}})}
    \label{eq:elbo_grad_vpmc}
    \end{equation}
    \State  $j\leftarrow j+1$
    \EndWhile
    \State  $\theta^{*} \leftarrow \theta^{{j}}$
    \State  $\phi^{*} \leftarrow \phi^{{j}}$
    \label{line:end_dtmc_vpmc}
  \end{algorithmic}
    % \vspace*{0.2cm}
  \end{algorithm}


% \begin{example}
%     \label{example:dkl_gaussian}
%     Given two univariate Gaussian distributions $p(z)$ and $q(z)$,
%     with parameters $\mu_p$, $\sigma_p^2$ and $\mu_q$, $\sigma_q^2$ respectively. 
%     The KL divergence between  $p$ and $q$ is given by
% $$\dkl(p \parallel q) = \frac{1}{2} 
% \left( \log \left( \frac{\sigma_q^2}{\sigma_p^2} \right) 
% + \frac{\sigma_p^2 + (\mu_p - \mu_q)^2}{\sigma_q^2} - 1 \right) \text{.}$$
% \end{example}

On the other hand, the term $\L_1(\theta,\phi)$~\eqref{eq:elbo_vpmc_l1} 
coincides with
expectations w.r.t.~$\q$ and can be approximated by Monte Carlo estimation.
% with samples $\latent_{0:T}^{(m)} \sim \q(\latent_{0:T}|\obs_{0:T})$, for $m \in [1:M]$.
For this, we use the reparameterization trick
presented in Subsection~\ref{subsec:reparameterization_trick},
which can be extended to the sequential case by considering
Equation~\eqref{eq:reparameterization_trick} for each time 
step $t$, as follows:
\begin{align}
    \label{eq:reparametrization_vpmc}
    \latent_{0:T}^{(m)} =& g(\phi, \epsilon_{0:T}^{(m)}) \text{, for } m \in [1:M] \text{.}
\end{align}
% where $\epsilon^{(m)}$ is a random variable sampled from a distribution
% which does not depend on $\phi$, and where $g$ is a differentiable function of $\phi$.\\
Thus, $\L_1(\theta,\phi)$~\eqref{eq:elbo_vpmc_l1}
can be approximated by
% \begin{align}
%     \label{eq:elbo-gpmc-mc}
%     \hat{\Qgen}(\theta,\phi) &= -\frac{1}{M} \sum_{m=1}^M 
% \log \left(\frac{\q(\latent_{0:T}^{(m)}|\obs_{0:T})}{\p(\latent_{0:T}^{(m)},\obs_{0:T})}\right) \text{, } 
% \end{align}
\begin{align}
    % \label{eq:elbo-pmc-mc}
    % \hat{\Qgen}(\theta,\phi) =&  \;  \hat{\L}_1(\theta,\phi) +  
    % \L_2(\theta,\phi)\\
    % % \hat{\L}_2(\theta,\phi)\\
    \label{eq:elbo-pmc-mc-1}
    \hat{\L}_1(\theta,\phi) =& \frac{1}{M} \sum_{m=1}^M  \log \p(\obs_0| \latent_0^{(m)})
    +\frac{1}{M} \sum_{m=1}^M  \sum_{t=1}^T  
    \log \p(\obs_t|\latent_{t-1:t}^{(m)},\obs_{t-1} ) \text{,}
    % \text{,}\\
    % \hat{\L}_2(\theta,\phi) = & - \dkl(\q(\latent_0| \obs_{0:T} ) | |  \p(\latent_0))\nonumber \\
    % \label{eq:elbo-pmc-mc-2}
    % &- \sum_{t=1}^T  \dkl(\q(\latent_t|\latent_{0:t-1},\obs_{0:T}) | | \p(\latent_t|\latent_{t-1},\obs_{t-1})) \text{.}
\end{align}
where $\latent_{t}^{(m)}$ is a differentiable function of $\phi$ that is sampled from
$\q(\latent_{t}|\latent_{0:t-1},\obs_{0:T})$, for $m \in [1:M]$ and $t \in [0:T]$.
Algorithm~\ref{algo:algo_train_dpmc_gen} 
summarizes the general estimation algorithm
for  general PMCs. Here, we learn the generative model by maximizing the
ELBO $\Qgen$ with respect to their parameters $\theta$ and $\phi$.



% \begin{remark} 
%     In the case of DPMCs, the parameters $\phi$ of the variational distribution $\q$ are the 
%      parameters of the DNNs which are differentiable w.r.t. $\phi$. Thus, 
%      Algorithm~\ref{algo:algo_train_dpmc_gen} 
%      can be applied to DPMCs. 
%  \end{remark}

\section{Experiments and results}
In this section, we first introduce a particular instance of the PMC model
which combines the deep PMC model (see Section~\ref{sec:dpmc}) 
and the stochastic RNN model (SRNN)~\citep{bayer2015learning, chung2015recurrent}.
From this instance, we derive different generative models for sequential data
with specific dependencies between latent and observed variables.
Finally, we compare their performance with
the stochastic RNN model on two datasets.


\subsection{Model description}
SRNN  architectures are specific instances of the PMCs,
% (see Section~\ref{sec:pmc_unified_model}).
which have demonstrated good experimental results~\citep{bayer2015learning, chung2015recurrent}, 
making it natural to compare them with their PMC extension. 
We introduce a model that combines the DPMC, and 
the SRNN models.
This generative (deep) PMC model consists of a latent process 
in an augmented dimension,  $\Latent_t \leftarrow (\Lat_t,\Latent_t)$,
the transition distribution now reads 
\begin{align}
    \label{eq:tmc-param}
    \p(\lat_t,\latent_t,\obs_t|\lat_{t-1},\latent_{t-1},\obs_{t-1}) = &\; \p(\lat_t|\lat_{t-1},\latent_{t-1},\obs_{t-1}) \p(\latent_t|\lat_{t-1:t},\latent_{t-1},\obs_{t-1}) \times \nonumber \\
    & \; \; \p(\obs_t|\lat_{t-1:t},\latent_{t-1:t},\obs_{t-1}) \text{.}
\end{align}
\begin{remark}
    Note that the previous equation  is nothing more than a particular case of the TMC model
    with transition~\eqref{eq:tmc-param}.    
    However, we consider it as a particular instance of the PMC model
    since $\lat_t$ and $\latent_t$, f←or all $t \in [0:T]$, are considered as latent variables
    with no physical interpretation.
\end{remark}
On the other hand, the variational distribution $\q$ defined in~\eqref{eq:var_dist}
is factorized as follows
\begin{align*}
    % \label{eq:var_pmc_latent}
    \q(\latent_{t}, \lat_{t} | \latent_{0:t-1},  \lat_{0:t-1},\obs_{0:T}) &= 
    \q(\latent_{t} | \latent_{0:t-1},  \lat_{0:t},\obs_{0:T}) 
    \q(\lat_{t} | \latent_{0:t-1},  \lat_{0:t-1},\obs_{0:T}) \text{.}  %\\
    % &= \q(\latent_{t} |\lat_{0:t},\obs_{t}) 
    % \q(\lat_{t} | \latent_{0:t-1},  \lat_{0:t-1},\obs_{0:T})
\end{align*}

% \paragraph{Parameterization - }
We consider the general parameterization presented in 
Subsection~\ref{sec:pmc_unified_model}. 
However, we now have an additional distribution because of the new variable $\lat_t$.
Let $\lambda$ be a distribution on $\lat_t$ parameterized by a differentiable (w.r.t. $\theta$) and 
vector valued function denoted as $\ph$ and which can depend on 
$(\lat_{t-1},\latent_{t-1},\obs_{t-1})$.
% % \ref{sec:pmc_unified_model}
% \katy{Aqui creo que es mejor que consideramos las mismas hipotesis presentadas en la param. general
% y asi solo anado una nueva funcion $\lambda$ para la nueva variable introducida}
% \katy{Change this parte wrt Yohan HDR because of the use of Z, H, and X and not $x_t$ and so on:
% Let $\lambda$, $\zeta$ and $\eta$  be distributions on $\lat_t$, $\latent_t$ and $\obs_t$, respectively,}
% and parameterized by differentiable (w.r.t. $\theta$) and vector valued functions denoted as
% $\ph$, $\pz$ and $\px$
% and which can depend on $(\lat_{t-1},\latent_{t-1},\obs_{t-1})$,
% $(\lat_{t-1:t},\latent_{t-1},\obs_{t-1})$ and on $(\lat_{t-1:t},\latent_{t-1:t},\obs_{t-1})$, respectively.
We recall that $\pz$ and $\px$ are defined in~\eqref{pmc-theta-1-gen}
and~\eqref{pmc-theta-2-gen}, respectively.
Thus, the parameterized transition~\eqref{eq:tmc-param} reads
\begin{equation*}
    % \label{eq:tmc-theta}
    \begin{aligned}
    \p(\lat_t|\lat_{t-1},\latent_{t-1},\obs_{t-1})  &= \lambda\left(\lat_t; \ph(\lat_{t-1},\latent_{t-1},\obs_{t-1})\right) \text{,} \\
    %\label{tmc-theta-2}
    \p(\latent_t|\lat_{t-1:t},\latent_{t-1},\obs_{t-1})&=\eta \left(\latent_t;\pz(\lat_{t-1:t},\latent_{t-1},\obs_{t-1})\right) \text{,}\\
    %\label{tmc-theta-3}
    \p(\obs_t|\lat_{t-1:t},\latent_{t-1:t},\obs_{t-1})&=\zeta\left(\obs_t;\px(\lat_{t-1:t},\latent_{t-1:t},\obs_{t-1}) \right) \text{.}
    \end{aligned}
\end{equation*}

In the context of SRNN architectures,
the variable $\lat_t$ represents a deterministic summary of the
past until time $t-1$, \ie~$\lat_t = \ph(\lat_{t-1},\latent_{t-1}, \obs_{t-1})$.
While $\latent_t$ corresponds to a
noisy version of $\lat_t$ (it is why we have split
the latent process in two).
Note that since $\Lat_{0:T}$ is deterministic given $(\latent_{0:T},\obs_{0:T})$,
its posterior distribution becomes trivial, and thus 
there is no need to consider a variational distribution for it.
The variational distribution $\q$ is  then parameterized as
\begin{align}
    \label{eq:pmc_qz}
    \q(\latent_t | \latent_{0:t-1},  \lat_{0:t},\obs_{0:T}) &= 
    \q(\latent_t | \lat_{t},\obs_{t})
    = \tau(\latent_t; \qz (\lat_{t},\obs_{t})) \text{,}
\end{align}
where $\tau(\latent;\qz)$ is a probability density function
on $\mathbb{R}^{d_\latent}$ 
whose parameters are given by $\qz$, 
which is differentiable w.r.t. $\phi$.
% $\qz$ is a differentiable vector valued function parameterized by $\phi$.
Following this reasoning and with a slight abuse of notation 
(where $\lambda$ coincides with the Dirac measure), we can incorporate several
degrees of generalization of the classical RNN  and of the SRNN
of \citet{chung2015recurrent}. 
The different deep PMC models we consider are defined in 
Table~\ref{tab:config-pmc} 
and are based on the specific dependencies of the involved random variables. 
Note that $\px$, $\ph$, $\pz$ and $\qz$ are now neural networks.

\begin{table}[!htpb]
    \begin{center}
    \small
    \begin{tabular}{|l|r|r|r|r|}
    % \begin{tabular}{l|rrr|}
    \cline{2-4}
    \multicolumn{1}{l}{}             & \multicolumn{3}{|c|}{\textbf{Parameterized function}}                                     \\ \hline
        \multicolumn{1}{|l|}{\textbf{Models}} & \multicolumn{1}{r|}{$\ph$} & \multicolumn{1}{r|}{$\pz$} & $\px$                 \\ \hline
    % \multicolumn{1}{|c|}{\diagbox{Model} {Parameterized function}}  & \multicolumn{1}{c|}{$s_{\theta}$} & \multicolumn{1}{c|}{$f_{\theta}$} & \multicolumn{1}{c|}{$g_{\theta}$} \\ \hline 
    RNN    &  $(\lat_{t-1},\obs_{t-1})$ & $\times$ & $\lat_t$ \\ \hline
    SRNN    &  $(\lat_{t-1},\latent_{t-1},\obs_{t-1})$ & $\lat_t$ & $(\lat_t,\latent_t)$ \\ \hline
    PMC-I    & $(\lat_{t-1},\latent_{t-1},\obs_{t-1})$  & $\lat_t$   & $(\lat_t,\latent_t,\obs_{t-1})$  \\ \hline
    PMC-II  & $(\lat_{t-1},\latent_{t-1},\obs_{t-1})$  &$\lat_t$ &  $(\lat_{t-1:t},\latent_{t},\obs_{t-1})$  \\ \hline
    PMC-III &  $(\lat_{t-1},\latent_{t-1},\obs_{t-1})$  & $\lat_t$  &  $(\lat_{t-1:t},\latent_{t-1:t},\obs_{t-1})$ \\ \hline
    PMC-IV &  $(\lat_{t-1},\latent_{t-1},\obs_{t-1})$  & $(\lat_t,\obs_{t-1})$  & $(\lat_{t-1:t},\latent_{t},\obs_{t-1})$ \\ \hline
    \end{tabular}
    \end{center}
    \caption{Configuration of the dependencies for different deep generative \gls*{pmc}s. 
    In each model, the sequence of latent variables $\{\Lat_t\}_{t \in \NN}$ 
    is treated as a deterministic variable given the observations. As a result, 
    $\eta$ coincides with the Dirac measure. The distribution $\lambda$ 
    is typically chosen to be Gaussian,
    while $\zeta$ depends on the nature of the observations. 
    Remember that in a classical \gls*{rnn}, $\{\Latent_t\}_{t \in \NN}$ 
    is not considered.}
    \label{tab:config-pmc} 
\end{table}


\subsection{Results}

% \subsection{Experiments}
\paragraph{Model configuration -}
In our experiments, the observed random variables are discrete,
and each $\Obs_t$ takes values in a binary space $\{0,1\}^{d_{\obs}}$.
% \katyobs{the last notation is correct?}
As a consequence, the distribution $\zeta$ coincides with the
Bernoulli distribution, and the output of $\px$ with its parameter.
For $\lambda$, we choose the Gaussian distribution and 
the output of $\pz$ corresponds to the mean and to the diagonal covariance matrix of 
the Gaussian distribution, which is summarized as follows
\begin{equation*}
    \begin{aligned}
    \lat_t &= \ph(\; \cdot \;) \text{,} \\
    \p(\latent_t|\; \cdot \;)  &= \N\left(\latent_t;  \mulatentp , \diag(\siglatentp) \right)   \text{, where }   \left[ \mulatentp , \siglatentp \right]= \pz(\; \cdot \;) \text{,} \\
    \p(\obs_t|\; \cdot \;)&= \Ber\left(\obs_t; \ropx \right) 
    \text{, where } \ropx  = \px(\; \cdot \;) \text{.}
    \end{aligned}
\end{equation*}
Here the notation $( \; \cdot \;)$ is used to avoid presenting a specific dependence between variables. 
These dependencies are specified for each model and are presented in 
Table~\ref{tab:config-pmc}.
The variational distribution $\q$ given in~\eqref{eq:pmc_qz}
is chosen as Gaussian, which satisfies
% \begin{align}
%     \label{eq:m3}
%     % \q(\latent_{t}|\lat_{0:t-1},\obs_{0:T}) &= \mathcal{N}\left(\latent_t;\qz(\lat_t,\obs_t)\right) \text{,}
%     \q(\latent_t|\lat_{t-1:t},\latent_{t-1},\obs_{t})&= \mathcal{N}\left(\latent_t;\qz(\lat_t,\obs_t)\right) \text{,}
% \end{align}
\begin{equation*}
    % \q(\latent_{t}|\latent_{0:t-1},\obs_{0:T}) = 
    \q(\latent_t|\lat_{t}, \obs_{t})  =
    \mathcal{N}\left(\latent_t; \mulatent, \diag(\siglatent) \right) \text{, where } 
    \left[ \mulatent, \siglatent \right] = \qz(\lat_{t},\obs_t) \text{.}
\end{equation*}
The functions   $\px$, $\pz$, $\ph$ and $\qz$ are implemented as
neural networks consisting of two hidden layers.
The rectified linear unit (ReLu) activation function is used for the hidden layers,
and the outputs of the neural networks are adapted according to their role. 
For example, the output of $\px$ is a layer of $d_\obs$ sigmoid functions 
due to the nature of the observations (binary values).
Additionally, the number of hidden units of each neural network coincides with the dimension $d_\latent$ 
of $\Latent_t$ and is different for each model and data set, which is specified in the next part.



\paragraph{Training - }
Each model was trained with stochastic gradient descent on the negative evidence 
lower bound using the Adam optimizer~\citep{kingma2014adam} with 
a learning rate of $0.001$.
% and a batch size of $512$ images.
% \katyobs{Check the batch size for both data sets}
The number of epochs was set to $100$ for both data sets.
% We have two configurations for the training of the models:
The number of hidden units of the neural networks ($d_\latent$) can be fixed for all the models,
or can be chosen by considering the number of parameters of the models to be compared
(\ie~the number of parameters are the same or close to). 

\paragraph{Evaluation - } 
The  performance of the models is evaluated in terms of the approximated ELBO
and log-likelihood of the observations on the test data set; 
we use a particle filter with the estimated variational distribution 
as importance distribution and $N=100$ particles.
% , see Algorithm~\ref{alg:particle_filter}.

% \katy{INTRODUCTION: Add the algorithm of the particle filter in Main Concepts section }


\paragraph{Image generation - }
The MNIST dataset~\citep{lecun1998mnist} contains  $60000$ (resp. $10000$) train (resp. test) 
$28 \times 28$  binary images. In this case, 
an observation $\Obs_t$ consists of a column of the image
and its dimensionality is $d_\obs=28$. The length of each sequence is $T+1=28$.
For this data set, we consider two configurations for the training of the models and are
summarized in Table~\ref{tab:config}. Config. 1 corresponds to the configuration
in which the number of hidden units of the neural networks is fixed $d_\lat=100$ for all the models. 
In Config.2, the number of hidden units of the neural networks
is chosen by considering the number of parameters of the models to be compared. 
We set $d_\lat=162$ , $d_\lat=100$, $d_\lat=95$, $d_\lat=79$, $d_\lat=78$ and 
$d_\lat=74$  for the RNN, SRNN, the
PMC-I, the PMC-II, the PMC-III, and the PMC-IV, respectively.  
We also set $d_\latent=3$ for each model and both configurations.

\begin{table}
    \begin{center}
        \begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|}
        \hline
        % \multirow{2}{*}{\diagbox{Model}{Data set}} 
        \multirow{3}{*}{Model}  &\multicolumn{4}{c|}{MNIST data set}  &\multicolumn{2}{c|}{\!\!Music data sets\!\!} \\
        \cline{2-7}
           &\multicolumn{2}{c|}{Config. 1} &\multicolumn{2}{c|}{Config. 2} &\multicolumn{2}{c|}{Config. 2}  \\
        % & \multicolumn{2}{c|}{MIDI} \\ 
        \cline{2-7}
         & \multicolumn{1}{c|}{$d_{\latent}$} & \multicolumn{1}{c|}{$d_{\lat}$}  & \multicolumn{1}{c|}{$d_{\latent}$} & \multicolumn{1}{c|}{$d_{\lat}$}   & \multicolumn{1}{c|}{$d_{\latent}$} & \multicolumn{1}{c|}{$d_{\lat}$}  
        % & \multicolumn{1}{c|}{$d_{\lat}$} & \multicolumn{1}{c|}{$d_{\latent}$}
        \\ \hline \hline
        RNN     & 3 & 100 & 3 & 162 & 300 & 562  \\ %\hline
        SRNN    & 3 & 100 & 3 & 100 & 300 & 300  \\ %\hline
        PMC-I   & 3 & 100 & 3 & 95 & 300 & 294  \\ %\hline
        PMC-II  & 3 & 100 & 3 & 79 & 300 & 278  \\ %\hline
        PMC-III & 3 & 100 & 3 & 78 & 300 & 260 \\ %\hline
        PMC-IV  & 3 & 100 & 3 & 74 & 300 & 272  \\ \hline
        \end{tabular}    
        \end{center}
        %\vspace{-0.4cm}
        \caption{Dimensions of latent variables for each Deep PMC.  
        $\ph$, $\pz$, $\px$  and $\qz$ are implemented as 
        neural networks with two hidden layers. 
        The number of neurons on each layer coincide with $d_{\lat}$.}        
    \label{tab:config}
\end{table}

Table \ref{tab:t1} presents the averaged ELBO 
and the averaged approximated log-likelihood  on the test set assigned by our models.
The results with the Config.1 (resp. Config. 2) 
show that PMC-IV (resp. PMC-II) has the higher averaged ELBO and 
averaged approximated log-likelihood compared to other models.
This indicates that the performance of the PMCs is better than of SRNN and RNN models.
An example of images generated from the estimated $\p(\obs_{0:t})$ of the PMC-II 
is shown in Figure~\ref{fig:images}.
  
\begin{table}[!htpb]
    % \small
    \begin{center}
    \begin{tabular}{|l|r|r|r|r|}
        \hline
        
        \multirow{2}{*}{Model}     &\multicolumn{2}{c|}{MNIST, config. 1}&\multicolumn{2}{c|}{MNIST, config. 2}\\ 
        \cline{2-5} 
    % \multicolumn{1}{|c|}{Model}   
    & \multicolumn{1}{c|}{ELBO} & \multicolumn{1}{c|}{approx. LL } & \multicolumn{1}{c|}{ELBO} & \multicolumn{1}{c|}{approx. LL } \\ 
    \hline \hline
    RNN    & -65,976 & -65,976& -65,700 & -65,700 \\ %\hline
    SRNN    & -67,248 & -64,760 & -67,222 & -64,762 \\ %\hline
    PMC-I   & -66,544 & -64,076 & -67,322 & -64,698 \\ %\hline
    PMC-II  & -66,784 & -64,201 & \textbf{-66,815} & \textbf{-64,255} \\ %\hline
    PMC-III & -66,518 & -63,876 & -67,513 & -64,876 \\ %\hline
    PMC-IV & \textbf{-66,150} & \textbf{-63,603} & -67,648 & -64,924 \\ \hline
    \end{tabular}    
    \end{center}
    %\vspace{-0.4cm}
    \caption{Averaged ELBO and approximated log-likelihood (approx. LL) of the observations 
    on the test set with two different configurations. 
    For the RNN, the ELBO coincides with the (exact) log-likelihood.}
    \label{tab:t1}
\end{table}


\begin{figure}[!htpb]
    \centering
    \centerline{\includegraphics[width=5.5cm]{Figures/mnist_digits.PNG}}
  \caption{Examples of generated images from estimated $\p(\obs_{0:t})$ for
  the MNIST  data set with the PMC-II model.}
  \label{fig:images}
  \end{figure}


\paragraph*{Polyphonic music generation - }
We also consider the polyphonic music data sets~\citep{bengio2013advances},
% \katyobs{Check this reference}
where three polyphonic music data sets are available, the classical piano music 
(Piano), the folk tunes (Nottingham) and the four-part chorales by J.S. Bach (JSB). 
The input consists of $88$ binary visible units that span the whole 
range of piano from A0 to C8  (\ie~$\obs_t \in \{0,1\}^{88}$).
In this case, we consider the Config. 2 for the training of the models (see Table~\ref{tab:config})
since it is a fairer comparison between the models.

We set $d_\latent=300$  for each model, and  $d_{\lat}=562$, 
$d_{\lat}=300$,  $d_{\lat}=294$, 
$d_{\lat}=278$, $d_{\lat}=260$ and  $d_{\lat}=272$ for the RNN, the SRNN, the
PMC-I, the PMC-II, the PMC-III and  the PMC-IV respectively. 
Table \ref{tab:t2} presents the results of the averaged ELBO and the averaged approximated log-likelihood
where the PMC-II (resp. PMC-IV) has the best performance compared to other models
on the Piano (resp. Nottingham and JSB) data set.


\begin{table}[!htpb]
    \begin{center}
    % \small
    \begin{tabular}{|l|r|r|r|r|}
    \hline
    \multirow{2}{*}{Model}  &\multicolumn{3}{c|}{Polyphonic music data sets}\\ 
    \cline{2-4} 
      & \multicolumn{1}{c|}{Piano} & \multicolumn{1}{c|}{Nottingham} & \multicolumn{1}{c|}{JSB} \\ 
      \hline \hline
    RNN    &  -10,52 & -23,89 & -10,77 \\ %\hline
    SRNN    &  -9,4011 & -13,2982 & -10,2739 \\ %\hline
    PMC-I    & -9,3077  & -11,3856   &-10,3126  \\ %\hline
    PMC-II  & \textbf{-8,8265}   &-14,8485 &  -10,2409  \\ %\hline
    PMC-III &  -9,2285  &-13,3900  &  -10,1103 \\ %\hline
    PMC-IV &  -9,4134  & \textbf{-10,6323}  & \textbf{ -9,2372} \\ \hline
    \end{tabular}
    \end{center}
    \caption{Approximated likelihoods on the polyphonic music data sets.
    For the RNN, the exact log-likelihood is computed.}
    \label{tab:t2} 
\end{table}


\section{Generative power of PMCs}
In this section, our objective is to analyze the previous models from a
theoretical point of view. We consider a linear and stationary Gaussian PMC, with
$d_\obs=1$.
 In a stationary Gaussian process, the statistical properties, like the mean and
covariance of the observations, do not change over time. This stationarity implies that the
covariance between two observations depends only on the time difference $k$ between
them. This analysis is then based on the associated covariance function
$ r_k = \Cov(\obs_{t},\obs_{t+k})$, for all $k \in \NN$, which
characterize the distribution $\p(\obs_{0:T})$ induced by each model.
% \katyobs{Here change}

% In this section, our objective is to introduce distributions $\p(\obs_{0:T})$ 
% induced by a linear and stationary Gaussian PMC.
% Such distributions are characterized by their
% covariance sequence 
% % specifically focusing on the covariance function 
% % $r_k = \Cov(\obs_{t},\obs_{t+k})$, for all $k \in \NN$. 
% % However, a direct comparison is not possible as $r_k$ lacks 
% % a closed-form expression without additional assumptions on the model.\\
% % To that end, 
% We first introduce a linear and stationary Gaussian PMC 
% with $d_\obs = 1$. 
% Next, we derive a closed-form expression for the covariance sequence $\{r_k\}_{k \in \NN}$.
% % This expression is then compared to the covariance of the linear and stationary Gaussian GUM.
% This will facilitate a theoretical analysis of the modeling
% power of the PMC.


\subsection{
    Linear and stationary Gaussian PMCs
}
\paragraph{Linear PMC -}
We consider the  case where  $\pz$ and $\px$ in
equations~\eqref{pmc-theta-1-gen}-\eqref{pmc-theta-2-gen} are 
vectorial linear  functions.
We have the following linear parameterization of the PMC
\begin{align}
    \label{eq:linear_pmc1}
    \p(\latent_0, \obs_0) &= \varsigma \big( (\latent_0, \obs_0); \;[ 0 ;  \Sigma_0 \;] \big) \text{,}\\
    \label{eq:linear_pmc2}
    \p(\latent_t|\latent_{t-1},\obs_{t-1})&= \eta(\latent_t; \;[ a\latent_{t-1}+ c\obs_{t-1}; \alpha ])  \text{,} \\%\quad \text{for all } t\in [1,T] \text{,}\\
    \label{eq:linear_pmc3}
    \p(\obs_t|\latent_{t-1:t}, \obs_{t-1})&= \zeta(\obs_t; \;[ b\latent_{t} + e \latent_{t-1}+f\obs_{t-1} ; \beta ])\text{,} %\quad \text{for all } t\in [0,T] \text{,}
\end{align}
where the notation $[\cdot \; ; \; \cdot]$
considers the first and second order of the initial distribution $\p(\latent_0, \obs_0)$,
of  $\p(\obs_t | \latent_{t-1:t}, \obs_{t-1})$,
and of $\p(\latent_t | \latent_{t-1}, \obs_{t-1})$.
The dimensions of the parameters $a$, $b$, $c$, $e$ and $f$ % and $\tilde{\gamma}$ 
are 
$d_{\latent}  \times  d_{\latent}$,
$1  \times  d_{\latent}$,
$d_{\latent}  \times  1 $,
$1 \times  d_{\latent}$, 
$1 \times 1 $,
respectively.
The covariance matrix $\alpha$ is a square matrix and $\beta \geq 0$.
In the initial distribution  $\p(\latent_0, \obs_0)$, $0$ 
is a $(d_{\latent} + 1 )$ zero vector and $\Sigma_0$ is a $(d_{\latent} + 1 )$
square covariance matrix given by
\begin{align*}
    \Sigma_0 = \begin{bmatrix} \eta & \tilde{\gamma}^\intercal \\ \tilde{\gamma}& r_0 \end{bmatrix} \text{.}
\end{align*}
The dimensions of $\eta$ and $\tilde{\gamma}$ are $d_{\latent}  \times d_{\latent} $ and $1 \times  d_{\latent}$,
respectively; and $r_0$ is scalar.
Thus, the set of parameters now is $\theta=(a,b,c,e,f,\alpha,\beta,\eta,\tilde{\gamma}, r_0)$.


\paragraph{Gaussian PMC -}
We now consider $\varsigma$, $\eta$ and $\zeta$ as Gaussian distributions so
the distribution $\p(\obs_{0:T})$ is a multivariate Gaussian distribution due to
the linear structure of the model. However, it is important to note that this
assumption does not result in any loss of generality; we employ it here for the
sake of clarity.
The covariance function $r_k$ associated
to $\p(\obs_{0:T})$ can be deduced from the covariance matrix 
$\Sigma_t$ associated to the distribution $\p(\latent_t,\obs_t)$.
First, an equivalent representation of~\eqref{eq:linear_pmc1}
-\eqref{eq:linear_pmc3}
is obtained by considering the first and second
 order moments of the pair $(\latent_t,\obs_t)$ given $(\latent_{t-1},\obs_{t-1})$. 
Since this distribution involves the product of two Gaussian 
distributions, one being linear
in the other and with results on
conditional Gaussian distributions, we obtain:
% do the product of two gaussian pdf and one is linear in the other so using
% conditional results on gaussian, the pair remain gaussian and its parameters
% read
% \katyobs{check the change of notation you did} 
\begin{align*}
    %\label{eq:expec_pmc}
    &\E\left( \begin{bmatrix} \latent_t ,  \obs_t \end{bmatrix}^\intercal | \latent_{t-1},\obs_{t-1}  \right)= 
    M \begin{bmatrix} \latent_{t-1} \\ \obs_{t-1} \end{bmatrix} \text{, }\\
    %\label{eq:cov_pmc}
    &\Var\left( \begin{bmatrix} \latent_t , \obs_t \end{bmatrix}^\intercal | \latent_{t-1},\obs_{t-1}  \right)= \Sigma_{t|t-1} \text{,}
\end{align*}
where
\begin{equation}
    \label{eq:trans-var-pmc}
    M= \begin{bmatrix} a & c \\ ba+e & bc+f \end{bmatrix} \text{, } \quad \Sigma_{t|t-1} = \begin{bmatrix} \alpha & \alpha b^\intercal \\ b\alpha & \beta+ b\alpha b^\intercal \end{bmatrix} \text{.}
\end{equation}
The covariance of the pair $(\latent_t,\obs_t)$
is given by $\Sigma_0 \times  (M^k)^\intercal$, which
can be easily deduced from the previous representation.
We also obtain the following expression for the covariance matrix 
associated to the distribution $\p(\latent_t,\obs_t)$
 $\Sigma_t$,
\begin{equation}
    \label{eq:recursion_sigma}
    \Sigma_t  = M \Sigma_{t-1} M^\intercal + \Sigma_{t|t-1} \text{,}
\end{equation}
% $$ \Sigma_t = M \Sigma_0 M^\intercal + \Sigma_{t|t-1} \text{,}$$
which is an immediate consequence %of the application
of the Lemma~\ref{prop:gaussian} 
in Appendix~\ref{chap:appendix}.
% in Appendix~\ref{sec:proofs}. 
% $$\Cov (\begin{bmatrix} \latent_t , \obs_t \end{bmatrix}^\intercal, \begin{bmatrix} \latent_{t+k} , \obs_{t+k} \end{bmatrix}^\intercal)
% =  \Sigma_0 \times  (M^k)^\intercal$$

    



\paragraph{Stationary PMC - }
In order to assure the stationarity of $\{\obs_t\}_{t \in \NN}$,
we consider directly that the process $\{\latent_t,\obs_t\}_{t\in \NN}$
is stationary.
Consequently, $\Sigma_0$  and $\Sigma_t$~\eqref{eq:recursion_sigma}
should satisfy the following equivalence
%  $\Var(\begin{bmatrix} \latent_0 , \obs_0 \end{bmatrix}^\intercal) = \Sigma_0=\Sigma_{t}$, 
% where $\Sigma_{t}$ is given in~\eqref{eq:recursion_sigma}. 
% The following
% expression holds for all $t \in \NN^*$, 
\begin{equation}
    \label{eq:constraints-pmc}
    \Sigma_0= M \Sigma_0 M^\intercal + \Sigma_{t|t-1} \text{.} 
\end{equation}  
This matrix equation describes 
a set of constraints on the parameters of the PMC model, 
which ensures the stationarity of the distributions
$\p(\latent_t,\obs_t)$ and $\p(\obs_t)$.
%the observed sequence $\{\obs_{t}\}_{t \in \NN}$.
% the process $\{\latent_t,\obs_t\}_{t\in \NN}$
% and then of $\{\obs_{t}\}_{t \in \NN}$.


% Since  $(\latent_t,\obs_t) | (\latent_{t-1},\obs_{t-1})  \sim \N( (\latent_{t},\obs_{t1}); \; M \begin{bmatrix} \latent_{t-1} \\ \obs_{t-1} \end{bmatrix}, \Sigma_{t|t-1})$, 
% then $\Cov ((\latent_t,\obs_t), (\latent_{t+k},\obs_{t+k}))$  
% is given by  $\Sigma_{z_t} (M^{\tau})^\top$ where $M$ and $\Sigma_{z_t}$ are defined in Equation \eqref{eq:matM} and \eqref{eq:sigma_zt}, respectively.\\
% Model \eqref{eq:pmc_gen_t} satisfies
% \begin{align}
%     \label{eq:h0}
%     \p(\latent_0,\obs_0)                    & = \N \left(\begin{pmatrix} \latent_0 \\ \obs_0  \end{pmatrix};\begin{bmatrix} 0 \\ 0  \end{bmatrix} , \begin{bmatrix} \eta & \gamma \eta \\ \gamma \eta & 1 \end{bmatrix} \right) \\
%     \label{eq:ht}
%     \p(\latent_t | \latent_{t-1} , \obs_{t-1} )   & = \N (\latent_t; a\latent_{t-1} + c\obs_{t-1} , \alpha ) \text{,}                                                                                                                       \\
%     \label{eq:xt}
%     %\nonumber
%     \p(\obs_t | \latent_{t-1:t} , \obs_{t-1} ) & = \N (\obs_t; b \latent_t + e\latent_{t-1} + \! f\obs_{t-1} , \beta )\text{,}
% \end{align}
% where $\theta=(a,b,c,e,f,\alpha,\beta,\eta,\gamma)$.
% The linear and Gaussian SRNN coincides with $e=f=0$, $\gamma=b$,
% while the linear and Gaussian \gls*{hmc} also satisfies $c=0$.

% \begin{remark}
%     Note that the linear HMC, RNN and GUM are particular cases of the linear PMC.
%     In particular, if we set
%     $\Cov(\latent_0,\obs_0)=\eta b^T$, 
%         \begin{itemize}
%             \item the HMC is obtained by setting $c=0$, $e=0$, $f=0$,
%             \item in an RNN, the transition between $(\latent_{t-1},\obs_{t-1})$ 
%             and $\latent_t$ is deterministic so $\alpha=0$. 
%             We have also set $e=0$, $f=0$. 
%             \item The GUM is obtained by setting $e=0$, $f=0$. 
%         \end{itemize}
%     \katyobs{Check this remark, add some details.}
% \end{remark}




    
% \begin{remark}
%     \yohan{not a good idea. I will put such a remark at the end, in the case
%     where you want to analyse d\_x >1; if we want to underline that the Gaussian
%     framework is not necessarily, you can put it in the introduction of the
%     section, by saying we consider gaussian distribution without loss of
%     generaility; the important this is the linear caracter of the model.}

%     The equations~\eqref{eq:trans-var-pmc} and~\eqref{eq:constraints-pmc}
%     can be obtained beyond the Gaussian assumption.
%     The problem relies on  $M$ and $\Sigma_{t|t-1}$, that are not explicitly given. 
%     This makes the direct deduction of  $r_k$
%     from the covariance of the joint distribution $\p(\latent_t,\obs_t)$ difficult.
%     To address this challenge, the stochastic realization theory can be 
%     introduced. This theory has been developed for the GUM in
%     a recent study~\citep{desbouvries2022expressivity}. %
%     % , which provides more details and results.
%     For the PMC, applying this theory can be more difficult due to the 
%     introduction of new dependencies through the parameters 
%     $(e,f)$.  However, a detailed exploration of this perspective is out
%     of the scope of this thesis.  Interested readers are 
%     encouraged to refer to \cite{desbouvries2022expressivity} for a comprehensive
%     analysis of the GUM, the HMC and the RNN.
%     % We refer the reader to Appendix~\ref{sec:stochastic_realization} 
%     % for some results based on the stochastic realization theory for the PMC;
%     % and~ for a detailed analysis of the GUM, the HMC and the RNN.
% \end{remark}
% % \yohan{With the stochastic realization theory, we can obtain the covariance function of the PMC.
% % but it is not necessarily a Gaussian distribution.}


\subsection{Theoretical analysis of PMCs}
% \katy{HERE}
We are interested in the modeling power of the PMC. 
For that, we characterize the covariance function
of the distribution $\p(\obs_{0:T})$ induced by the PMC,
and compare it with the one of the GUM presented in~\cite{salaun2019comparing}.
We focus on the case where the latent and observed variables 
are both scalar ($d_\latent = 1$ and $d_\obs = 1$).
The scalar case is interesting because it allows for a direct deduction of 
the covariance function derived from the PMC. 
% The PMC is described by \eqref{eq:trans-var-pmc}
% and satisfies \eqref{eq:constraints-pmc}.

For clarity, we set $r_0=1$, which means $\p(\obs_t)=\N(\obs_t;0;1)$,
for all $t \in \NN$. We also parameterize $\tilde{\gamma}=\gamma \eta$,
then the set of parameters is now given by $\theta=(a,b,c,e,f,\alpha,\beta,\eta,\gamma)$.
%             \item the HMC is obtained by setting $c=0$, $e=0$, $f=0$,
%             \item in an RNN, the transition between $(\latent_{t-1},\obs_{t-1})$ 
%             and $\latent_t$ is deterministic so $\alpha=0$. 
%             We have also set $e=0$, $f=0$. 
%             \item The GUM is obtained by setting $e=0$, $f=0$
% We start by presenting the covariance function of the GUM plugging the parameters
We start by presenting the covariance function of the GUM, 
% ($e=f=0$, and $\gamma=b$), 
where the matrix $M$ is diagonalizable. 
By plugging in $e=f=0$, and $\gamma=b$, the covariance function of the GUM is given by
% The covariance function of the GUM is given by
% With these assumptions, the matrix $M$ is diagonalizable in the GUM ($e=f=0$),
% and the covariance function is given by 
\begin{equation}
    \label{eq:covar-gum}
    \begin{aligned}
    % \Var(\Obs_t)&= \beta+b^2\eta \text{,} \quad \text{ for all } t \in \NN  \text{,}\\
    % % \Cov(\Obs_t,\Obs_{t+k})
    % \hat{r}
    % r_k = (a+cb)^{k-1} (a\eta b^2 + bc (\beta+ \eta b^2 )) \text{,} \quad \text{ for all } k \in \NN^*  \text{,}
    r_k & \overset{\text{GUM}}{=}  A^{k-1} B 
    % (\underbrace{a+cb}_{A})^{k-1} 
    % (\underbrace{a\eta b^2 + bc (\beta+ \eta b^2 )}_{B})
     \text{,} \text{ for all } k \in \NN^*  \text{,}
    \end{aligned}
\end{equation}
where $A=a+cb$, $B=a\eta b^2 + bc (\beta+ \eta b^2 )$,
and  $\Var(\Obs_t)= \beta+\eta b^2 = 1$, for all $t \in \NN$. 
The stationarity constraints are simplified to two constraints:
\begin{align*}
    \beta &= 1 - b^2 \eta \text{,}\\
    \alpha &= \big(1  - a^2 -2abc \big)\eta - c^2 \text{,}
\end{align*}  
In addition to the settings of the GUM, the covariance functions 
associated to a linear and stationary Gaussian HMC and RNN
are also derived.  
\begin{itemize}
\item \textbf{HMC -} with $c=0$ and  $r_k= a^{k} \eta b^2 $.
% which implies $A=a$ and $B=ab^2\eta$;
\item \textbf{RNN -} the transition between $(\latent_{t-1},\obs_{t-1})$ 
and $\latent_t$ is deterministic then $\alpha=0$. Moreover, 
$\latent_{0}=0$ and $\Obs_{0}$ is independent of $\latent_{0}$. 
Since $\Var(\Obs_t)= b^2 \beta+\eta = 1$, 
%Since in our framework 
%we first define $\p(\latentent_{0})$, we add an artificial observation $\Obs_{-1}$ such 
%that $\E(\Obs_{-1})=0$ and  $\Var(\Obs_{-1})=r_0$.
%Finally, since $\latentent_0=c\obs_{-1}$,
the constraint $\eta=c^2$ should also be satisfied to ensure that 
% to ensure that $\eta_t=\eta$ and 
$\Var(\Obs_t)=r_0=1$ for all $t \in \NN$. 
%the stationarity of $\{\Latent_t\}_{t \in \NN}$ and so of  $\{\Obs_t\}_{t \in \NN}$.
\end{itemize}




In order to extend this study for PMCs, we assume that 
$M$ is diagonalizable in the PMC, \ie~$M=PDP^{-1}$ with
\begin{eqnarray*}
    P &=& \begin{bmatrix} \frac{-a+bc+f+K}{2(ab+e)} & \frac{a-bc-f+K}{2(ab+e)} \\ 1 & 1  \end{bmatrix} \text{,} \\
    D&=& \begin{bmatrix} \frac {1}{2}(a+bc+f-K) & 0 \\ 0 & \frac {1}{2}(a+bc+f+K) \end{bmatrix} \text{,} \\
    P^{-1}&=& \begin{bmatrix} -\frac{ab+e}{K} & \frac{a-bc-f+K}{2K} \\ \frac{ab+e}{K} & \frac{-a+bc+f+K}{2K} \end{bmatrix} \text{,}
\end{eqnarray*}
where 
$$K=\sqrt{(a+bc+f)^2-4(af-ce)} \text{.} $$
Note that the condition $(a+bc+f)^2-4(af-ce) \geq 0$ is satisfied
since $M$ is diagonalizable. 
As a result,  we can deduce  $r_k$ for the PMC, which is summarized in the following proposition.
%  $r_k=\Cov(\obs_{t},\obs_{t+k})$, for all $k \in \NN$,
% which is summarized in the following proposition.

% From the previous relation, the following restrictions on the parameters are obtained:
% \begin{align*}
% % \label{eq:varcond1}
% \eta &= \alpha +  a\eta (a+2c\gamma) + c^2\\    
% % \label{eq:varcond2}
% \gamma \eta &= b \alpha +  (a^2b + 2abc \gamma + ae + af\gamma + ce\gamma)\eta  + (bc^2 + cf) \\
% % \label{eq:varcond3}
% 1 &= \beta + b^2 \alpha + ((ab + e)^2 + 2\gamma  (ab+e) (bc + f) )\eta+ (bc + f)^2
% \end{align*}

% \begin{equation}
%     \label{contrainte}
%     p(\obs_t)=\mathcal{N}(\obs_t;0;1) \text{, for all } 0 \leq t \leq T.
% \end{equation}

% We will show that the \gls*{pmc} can model more complex distributions than the \gls*{hmc}, \gls*{rnn} and SRNN, if the Linear and Gaussian assumptions are satisfied.
% The following proposition shows that the \gls*{pmc} can model more complex Gaussian distributions.
% \begin{proposition}
%     \label{prop-1}
%     Let $p(\obs_{0:T})$ be a Gaussian distribution satisfying for all positive
%     integers $T,t,k$
%     \begin{eqnarray}
%         \label{obs-pmm-e}
%         p(\obs_{0:T})&=&\mathcal{N}(\obs_{0:T};{\bf 0}; \tilde{\Sigma}) \text{,} \\
%         \label{cov-pmm-e}
%         \text{\rm cov}(\obs_{t},\obs_{t+k})&=& \left\{
%         \begin{matrix}
%             \scriptstyle
%             \noindent \tilde{A}^{k}     & \; \text{if } k \text{  is even} \\
%             \scriptstyle
%             {\tilde A}^{k-1} {\tilde B} & \; \text{otherwise.}
%         \end{matrix} \right. \text{,}
%     \end{eqnarray}
%     such that $\tilde{A}$ and $\tilde{B}$ defines
%     a valid covariance matrix.
%     Then for such $(\tilde{A},{\tilde B})$, it exists a set
%     of parameters $\theta=(a,b,c,e\neq 0,\alpha,\beta,\eta)$
%     such that $\p(\obs_{0:T})=p(\obs_{0:T})$.
% \end{proposition}
% This proposition shows that the linear and Gaussian PMM can model
% some Gaussian distributions which cannot be modeled by the previous
% linear and Gaussian \gls*{hmc}, \gls*{rnn} and SRNN.



\begin{proposition}
    \label{prop:cov}
    Let a linear and stationary (scalar) Gaussian PMC be defined by the transition and the conditional
    covariance matrices $M$ and $\Sigma_{t|t-1}$ in \eqref{eq:trans-var-pmc}
    and the initial covariance matrix 
    \begin{equation*}
    \Sigma_0 = \begin{bmatrix} \eta & \gamma \eta \\ \gamma \eta & 1 \end{bmatrix} \text{.} 
    \end{equation*}
    If $M$ is diagonalizable, the covariance function of $\{\Obs_t\}_{t \in \NN}$ reads 
    \begin{equation}
    \label{eq:cov-pmc}
    % \Cov(\Obs_{t},\Obs_{t+k})
    r_k= \overline{A}^{k} (\overline{B}+\frac{1}{2}) - \overline{C}^{k} (\overline{B}-\frac{1}{2})  \text{,}
    \end{equation}
    where 
    \begin{eqnarray*}
    \overline{A}&=& \frac{a + bc + f -K}{2}  \text{,} \\
    \overline{B} &=& \frac{a-bc-f-2 \gamma \eta (ab+e)}{2K}\text{,} \\
    \overline{C} &=&\frac{a + bc + f +K}{2} \text{,} \\
    K&=& \sqrt{(a+bc+f)^2 - 4(af  - ce )}
    \end{eqnarray*}
    and where the following stationarity constraints are satisfied:
    \begin{align*}
     b\eta+(ae+af\gamma+ce\gamma)+fc &=\gamma\eta \text{,}\\
     (1-a^2-2ac\gamma)\eta-c^2 & \geq 0 \text{,} \\
     1- b^2\eta- 2b\eta(\gamma-b)-e\eta(e+2f\gamma)-f^2 & \geq 0 \text{.}
    \end{align*}\\
\end{proposition}

\begin{proof}
    The proof relies on the assumption that  $M$ is diagonalizable, 
    which enables us to derive an explicit expression for the
    covariances of the pair $(\latent_t,\obs_t)$. 
    Then $r_k$ can directly be deduced from this expression that reads
    \begin{align*}
        % \label{eq:covariance}
        \Sigma_0 \times  (M^k)^\intercal & = \Cov (
            \begin{bmatrix} \latent_t , \obs_t \end{bmatrix}^\intercal, 
            \begin{bmatrix} \latent_{t+k} , \obs_{t+k} \end{bmatrix}^\intercal) \\
        &=\begin{bmatrix}
        \Cov(\latent_{t},\latent_{t+k}) &  \Cov(\latent_{t},\obs_{t+k})\\ 
        \Cov(\obs_{t},\latent_{t+k}) & \pmb{\Cov(\obs_{t},\obs_{t+k})}%=r_k}
        \end{bmatrix}\text{.}
    \end{align*}
    On the other hand, the stationarity constraints are given by \eqref{eq:constraints-pmc}.
    % , which were deduced by applying the Lemma~\ref{lem:recursion_covariance}.
    %  and the fact that the process $\{\latent_t,\obs_t\}_{t\in \NN}$ is stationary.
    We set $r_0=1$ and $\tilde{\gamma}=\gamma \eta$, so the following 
    stationary relation holds
    \begin{align*}
     \begin{bmatrix}
     \eta & \gamma \eta\\ 
     \gamma \eta & 1
    \end{bmatrix}  &=  \begin{bmatrix}
    \alpha & b\alpha \\ 
    b\alpha & \beta + b^2\alpha.
    \end{bmatrix} \; +    \begin{bmatrix}
    a & c\\ 
    ab+e & bc+f
    \end{bmatrix} 
     \begin{bmatrix}
    \eta &  \gamma \eta\\ 
    \gamma \eta & 1 \\ 
    \end{bmatrix}  
    \begin{bmatrix}
    a & ab+e\\ 
    c & bc+f
    \end{bmatrix}\text{.}
    \end{align*}
    Since the covariance matrix is symmetric and the diagonal elements are positive because
    they are variances, the set of 3 constraints are deduced directly from the previous relation.        
\end{proof}

\begin{remark}
    The stationarity of the distribution $\p(\obs_{0:T})$ 
    implies that
    its associated variance-covariance matrix $\Sigma^{\obs}_{T}$  is a Toeplitz matrix
    (\ie~the coefficients on each diagonal are equal) 
    fully determined by its first row given by the covariance  sequence
    $[r_0, r_1, \ldots,r_T]$, where $r_k$ is given by \eqref{eq:cov-pmc}, 
    for all $k \in \NN^*$.
    Thus, $\Sigma^{\obs}_{T}$   
    % depends on $\overline{A}$, $\overline{B}$ and $\overline{C}$
    reads as,  for all $T \in \NN^*$,

    % \begin{scriptsize}
        \begin{align*}
            % \displaystyle 
            % & \Cov(\Obs_{0:T},\Obs_{T})
            & \Sigma^{\obs}_{T}
            % (\overline{A}, \overline{B}, \overline{C})   
            \displaystyle  
            =
         \begin{bmatrix}
        1 & r_1 & r_2 & r_3 & \dots & r_T \\
        r_1 & 1 & r_1 & r_2 & \dots &\vdots \\
        r_2 & r_1 &  1 & r_1 &\ddots & \vdots\\
        r_3 & r_2&  r_1 & 1 &\ddots & \vdots\\
        \vdots &\ddots &  \ddots &\ddots &\ddots & \vdots\\
        r_T & \dots & r_3 & r_2 & r_1 & 1  
        \end{bmatrix}
        \text{.}
        \end{align*}
    % \end{scriptsize}
\end{remark}

%     \begin{scriptsize}
%         \begin{align*}
%             \displaystyle 
%             % & \Cov(\Obs_{0:T},\Obs_{T})
%             & \Sigma^{\obs}_{T}
%             % (\overline{A}, \overline{B}, \overline{C})   
%             \displaystyle  
%             \overset{PMC}{=} 
%             {\displaystyle \text{Toeplitz}
%             \big( [1, \overline{A} \big(\overline{B} + \frac{1}{2} \big) - \overline{C} \big(\overline{B} - \frac{1}{2} \big), \dots, 
%             \overline{A}^{T-1} \big(\overline{B} + \frac{1}{2} \big) - \overline{C}^{T-1} \big(\overline{B} - \frac{1}{2} \big) ]  \big)}
%             % \text{, for all } T \in \NN^* 
%             % \text{.} 
%             \\ \vspace*{0.4cm}
%             &= \begin{bmatrix}
%         1 & \overline{A}\big(\overline{B} + 
%         \frac{1}{2} \big)-\overline{C}\big(\overline{B} -\frac{1}{2} \big)
%         & \dots & \overline{A}^{T -1} \big(\overline{B} + \frac{1}{2} \big) 
%         - \overline{C}^{T -1} \big(\overline{B} - \frac{1}{2} \big) \\ 
%         \overline{A}\big(\overline{B} + \frac{1}{2} \big)-\overline{C}\big(\overline{B} - \frac{1}{2} \big)& 1 & \dots &\vdots \\
%         \vdots &\vdots &\ddots & \vdots\\
%         \overline{A}^{T-1} \big(\overline{B} + \frac{1}{2} \big) - \overline{C}^{T -1 } \big(\overline{B} - \frac{1}{2} \big) & \dots & \dots  &  1
%         \end{bmatrix}
%         \end{align*}
%     \end{scriptsize}
% \end{remark}


% \katy{Do I have to extend (proof and more details) this part similar to Achile's thesis ??????}


% \paragraph{Analysis - }
% The covariance functions associated to a linear and stationary 
% Gaussian  GUM, HMC and  RNN
% are particular cases of the covariance function of the PMC.
% % They are easily derived from the Proposition~\ref{prop:cov}
% % by setting $e=f=0$ and $\gamma=b$,
% % $c=0$ and $\alpha=0$, and $e=f=0$, respectively.
% For example, the covariance function $\{r_k\}_{k \in \NN}$ associated to 
% the linear and stationary
% Gaussian GUM~\eqref{eq:covar-gum}
%  is easily  derived from the Proposition~\ref{prop:cov}
% by setting $e=f=0$ and $\gamma=b$. 
% % Similarly, the covariance functions
% % associated to the linear and stationary Gaussian HMC and RNN
% % are also derived from the Proposition~\ref{prop:cov}
% % by setting $c=0$ and $\alpha=0$, respectively.

Proposition~\ref{prop:cov} shows that the PMC
generalizes the form of the covariance matrices of the GUM, HMC and RNN
by introducing the parameters $e$ and $f$.
However, it remains challenging to determine whether any covariance series 
in the form \eqref{eq:cov-pmc} can be generated by a PMC
because identifying $\overline{A}$, $\overline{B}$ and $\overline{C}$, 
in order to ensure that \eqref{eq:cov-pmc} represents a 
valid covariance series, is a complex problem. 
Nonetheless, we can exhibit some particular covariance functions 
that can be generated by a (particular) PMC but not by a GUM, HMC or RNN
as shown in the next proposition.


\begin{proposition}
    \label{prop:cov-pmc}
    Let $\tilde{A}$ and $\tilde{B}$ be two scalars, $r_0 =1$ and
    \begin{eqnarray}
    \label{eq:cov-pmm-e}
    r_k&=& \left\{
    \begin{matrix}
    \scriptstyle 
    \noindent \tilde{A}^{k}  & \; \text{if } k \text{  is even,}\\ 
    \scriptstyle
    {\tilde A}^{k-1} {\tilde B} & \; \text{otherwise.}
    \end{matrix} \right. %\text{.}
    \end{eqnarray}
    Then $\{r_k\}_{k \in \NN}$ is a covariance function if and only if
    \begin{equation}
    \label{eq:cond-A-B-tilde}
    -1  \leq \tilde A \leq  1 \quad \text{ and } \quad -\frac{ \tilde A^2 +1}{2} \leq \tilde{B} \leq \frac{ \tilde A^2 +1}{2} \text{,}
    \end{equation}
    and can be realized by a linear and stationary Gaussian PMC.\\
\end{proposition}


\begin{proof}
    The proof relies on the Carathéodory-Toeplitz
    theorem~\citep{akhiezer1965classical}  since $\Sigma^{\obs}_{T}$ is defined
    by a Toeplitz matrix with first row 
    $$[1,\tilde{B},\tilde{A}^2, \tilde{A}^2 \tilde{B}, \tilde{A}^4, \cdots].$$ 

    We analyze the series expansion of the covariance function to establish
    the necessary conditions for the positive semi-definiteness of $\Sigma^{\obs}_{T}$. 
    This theorem allows us to determine the values of $\tilde A$ and of
    $\tilde{B}$ in~\eqref{eq:cov-pmm-e} 
    such that $\Sigma^{\obs}_{T}$ is a valid covariance matrix.

    We deduce
    the constraints $-1  \leq \tilde A \leq  1$, 
    $ -\frac{ \tilde A^2 +1}{2} \leq \tilde{B} \leq \frac{ \tilde A^2 +1}{2} $. 
    Next, 
    % two cases provide the results exhibited in the proposition.
    setting $\gamma=b$, and~$f$ either as~$0$ or~$-a-bc$ (two particular cases of the PMC),
    we show that \eqref{eq:cov-pmc} coincides
    with \eqref{eq:cov-pmm-e}, with 
    \begin{eqnarray*}
        \left\{
       \begin{matrix}
       \tilde A=\sqrt{ce} \quad \text{ and } \quad \tilde{B}=b(c(1-b^2\eta)+e\eta)  & \;  \text{if } f = 0 \text{,} \\ 
       \tilde A=\sqrt{e^2\eta + a^2(1 -b^2\eta )}\quad \text{ and } \quad \tilde{B}= be\eta - a (1-b^2\eta) & \; \text{if } f = -a-bc  \text{.}
       \end{matrix} \right.
    \end{eqnarray*}
    Finally, for any $(\tilde A,\tilde{B})$ satisfying \eqref{eq:cond-A-B-tilde},
    we show that it is possible to find a set of parameters $(a,b,c,e,\eta,\alpha,\beta)$
    which satisfies the previous system and the stationarity constraints \eqref{eq:constraints-pmc}
    for both cases $f=0$ and $f=-a-bc$.
    For a detailed
    step-by-step proof, please refer to the Appendix~\ref{chap:appendix_22}.
\end{proof}

% \paragraph{Analysis -}
Proposition \ref{prop:cov-pmc} shows that it is possible 
to produce a covariance function
$$r_{k}=A^{k-1} B(k) \text{,}$$ 
with a switching $B(k)$ satisfying $B(k)=A$ if $k$ is even and
$B(k)=B$, otherwise.
The constraints on $A$ and $B$ with this switching are
$-1 \leq A \leq 1$ and $-\frac{A^2+1}{2} \leq B \leq \frac{A^2+1}{2}$ 
since $B(k)$ is as expression of $A$ and $B$. \\

This result can be compared with that of the GUM in the scalar 
case~\citep{salaun2019comparing}, 
that can produce any covariance function  given by~\eqref{eq:covar-gum},
$r_{k}=A^{k-1} B$,   with the constraints     $-1  \leq A  \leq 1 $ and 
$\frac{A-1}{2} \leq  B \leq \frac{A+1}{2}$.
In other words, this proposition shows that the linear and stationary Gaussian PMC
can model some Gaussian distributions which cannot be modeled by 
the previous linear and stationary Gaussian GUM.
% \katy{Check if it is correct and change the caption of the plot}
% This proposition shows that the linear and stationary Gaussian PMC 
% can model some Gaussian distributions which cannot be modeled 
% by the previous linear and stationary Gaussian GUM.
%-----------------------------------
% The following proposition shows that the linear and Gaussian PMC can model
% some Gaussian distributions which cannot be modeled by the previous
% linear and Gaussian HMC, RNN and GUM.


% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{Figures/Graphical_models/comp_prop.pdf}
%     \caption{The parallelogram (light blue) coincides with all the multivariate 
%     centered Gaussian distributions with a covariance matrix which satisfy
%     $\Cov(\Obs_t,\Obs_{t+k})=A^{k-1} B(k)$, for all $k \in \NN$. 
%     Such distributions can be modeled by a GUM.
%     \katy{Check if this interpretation is correct 
%     and change the caption of this plot}}
%     \label{fig:comp_pmc}
% \end{figure}


% The parallelogram (light blue) coincides with all the multivariate 
% centered Gaussian distributions with a covariance matrix which satisfy 
% Cov(Xt, Xt+τ ) = Aτ−1B. Such distributions can be modeled by a GUM.
% The blue (resp. orange) areas (resp. curves) coincide with the value of
% A and B which can be taken by the HMM (resp. the RNN). 
% This results shows that the modeling power of the GUM is larger than 
% that of the HMM, which is larger than that of the RNN. 
% However, let us note that in the context of this study, 
% the RNN is finally defined by only 2 free parameters, 
% the HMM relies on 3 parameters and the GUM on 4 parameters.

% \begin{figure}[htb]
%     \begin{subfigure}[b]{0.48\linewidth}
%       \centering
%       \includegraphics[width=6cm]{Figures/Graphical_models/gum_prop.pdf}
%       \caption{GUM}
%       \label{fig:gum_prop}
%       \vspace{1.1cm}
%     \end{subfigure}
  
%     \begin{subfigure}[b]{0.48\linewidth}
%       \centering
%       \includegraphics[width=6cm]{Figures/Graphical_models/pmc_prop.pdf}
%       \caption{PMC}
%       \label{fig:pmc_prop}
%     \end{subfigure}  
%     \caption{xx}
%     \label{fig:comparison}
%   \end{figure}
  






\section{Conclusions}
This chapter was devoted to the development, study, comparison 
and application of a general generative model for sequential data 
based on the PMC model.
Our approach combined the advantages of the HMM, RNN and GUM models 
and encapsulated them in a single framework.
A new parameter estimation method based on the variational inference
framework was also presented for the general PMC model, 
which is computationally efficient and easy to implement.
Moreover, we presented a particular instance of the variational PMC model,
combining the PMC model and deep parameterizations. This model has been compared
with the RNN and SRNN models on the MNIST and polyphonic music data sets. The
results show that the performance of the deep PMCs is better than of SRNN and
RNN models.
We have also shown that the linear and stationary Gaussian PMC
can model some Gaussian distributions which cannot be modeled by
the previous linear and stationary Gaussian HMC, RNN and GUM.
% We also showed that our framework can be used for 
% supervised sequential learning problems, 
% where the proposed parameter estimation method is still valid.


