\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Generative hidden Markov models}{29}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:pmc}{{2}{29}{Generative hidden Markov models}{chapter.2}{}}
\@writefile{toc}{\etoc@startlocaltoc{3}}
\citation{pieczynski2003pairwise}
\citation{rabiner1989tutorial}
\citation{medsker2001recurrent}
\citation{salaun2019comparing}
\citation{jaakkola2000bayesian,Blei_2017}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{30}{section.2.1}\protected@file@percent }
\@writefile{brf}{\backcite{pieczynski2003pairwise}{{30}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{rabiner1989tutorial}{{30}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{medsker2001recurrent}{{30}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{salaun2019comparing}{{30}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{jaakkola2000bayesian}{{30}{2.1}{section.2.1}}}
\@writefile{brf}{\backcite{Blei_2017}{{30}{2.1}{section.2.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The pairwise Markov chain as a unified model}{30}{section.2.2}\protected@file@percent }
\newlabel{sec:pmc_unified_model}{{2.2}{30}{The pairwise Markov chain as a unified model}{section.2.2}{}}
\citation{salaun2019comparing}
\newlabel{eq:pmc_gen_transition}{{2.1}{31}{The pairwise Markov chain as a unified model}{equation.2.2.1}{}}
\newlabel{eq:pmc_gen_t}{{2.2}{31}{The pairwise Markov chain as a unified model}{equation.2.2.2}{}}
\@writefile{brf}{\backcite{salaun2019comparing}{{31}{2.2}{equation.2.2.2}}}
\citation{salaun2019comparing}
\citation{chung2015recurrent}
\citation{fraccaro2016sequential}
\citation{Blei_2017}
\newlabel{fig:hmm}{{2.1a}{32}{HMC\relax }{figure.caption.12}{}}
\newlabel{sub@fig:hmm}{{a}{32}{HMC\relax }{figure.caption.12}{}}
\newlabel{fig:rnn}{{2.1b}{32}{RNN\relax }{figure.caption.12}{}}
\newlabel{sub@fig:rnn}{{b}{32}{RNN\relax }{figure.caption.12}{}}
\newlabel{fig:gum}{{2.1c}{32}{GUM\relax }{figure.caption.12}{}}
\newlabel{sub@fig:gum}{{c}{32}{GUM\relax }{figure.caption.12}{}}
\newlabel{fig:pmc}{{2.1d}{32}{PMC\relax }{figure.caption.12}{}}
\newlabel{sub@fig:pmc}{{d}{32}{PMC\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Conditional dependencies of the \gls *{hmc}, \gls *{rnn}, \gls *{gum}, and \gls *{pmc}. In the \gls *{rnn}, the hidden states $z_t$ are shown as diamonds to stress that they are no source of stochasticity. The \gls *{hmc}, \gls *{rnn}, and \gls *{gum} are particular cases of the \gls *{pmc}.\relax }}{32}{figure.caption.12}\protected@file@percent }
\newlabel{fig:graphical_models}{{2.1}{32}{Conditional dependencies of the \gls *{hmc}, \gls *{rnn}, \gls *{gum}, and \gls *{pmc}. In the \gls *{rnn}, the hidden states $\latent _t$ are shown as diamonds to stress that they are no source of stochasticity. The \gls *{hmc}, \gls *{rnn}, and \gls *{gum} are particular cases of the \gls *{pmc}.\relax }{figure.caption.12}{}}
\@writefile{brf}{\backcite{salaun2019comparing}{{32}{2.2.1}{remark.2.2.1}}}
\@writefile{brf}{\backcite{chung2015recurrent}{{32}{2.2.1}{remark.2.2.1}}}
\@writefile{brf}{\backcite{fraccaro2016sequential}{{32}{2.2.1}{remark.2.2.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Parameter estimation for general PMCs}{33}{section.2.3}\protected@file@percent }
\newlabel{sub:pmc_parameter_estimation}{{2.3}{33}{Parameter estimation for general PMCs}{section.2.3}{}}
\@writefile{brf}{\backcite{Blei_2017}{{33}{2.3}{section.2.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}General parameterization of PMCs}{33}{subsection.2.3.1}\protected@file@percent }
\newlabel{sec:pmc_parameterization}{{2.3.1}{33}{General parameterization of PMCs}{subsection.2.3.1}{}}
\newlabel{pmc-theta-1-gen}{{2.3}{33}{General parameterization of PMCs}{equation.2.3.3}{}}
\newlabel{pmc-theta-2-gen}{{2.4}{33}{General parameterization of PMCs}{equation.2.3.3}{}}
\newlabel{ex:gaussian}{{2.3.1}{33}{}{example.2.3.1}{}}
\citation{rumelhart1985learning,hecht1992theory}
\@writefile{toc}{\contentsline {paragraph}{Deep pairwise Markov chain - }{34}{example.2.3.1}\protected@file@percent }
\newlabel{sec:dpmc}{{2.3.1}{34}{Deep pairwise Markov chain - }{example.2.3.1}{}}
\@writefile{brf}{\backcite{rumelhart1985learning}{{34}{2.3.1}{example.2.3.1}}}
\@writefile{brf}{\backcite{hecht1992theory}{{34}{2.3.1}{example.2.3.1}}}
\newlabel{ex:dpmc_gaussian}{{2.3.2}{34}{}{example.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Illustration of a deep parameterization of the distribution~$p_{\theta }(z_t|z_{t-1},x_{t-1})$, where the parameters~$\mu _{\theta }^{z}$ and~$\sigma _{\theta }^{z}$ of the Gaussian distribution are the output of a DNN.\relax }}{34}{figure.caption.13}\protected@file@percent }
\newlabel{fig:dpmc_gaussian}{{2.2}{34}{Illustration of a deep parameterization of the distribution~$\p (\latent _t|\latent _{t-1},\obs _{t-1})$, where the parameters~$\mulatentp $ and~$\siglatentp $ of the Gaussian distribution are the output of a DNN.\relax }{figure.caption.13}{}}
\citation{kingma2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Variational Inference for PMCs}{35}{subsection.2.3.2}\protected@file@percent }
\newlabel{eq:elbo_general}{{2.5}{35}{Variational Inference for PMCs}{equation.2.3.5}{}}
\newlabel{eq:var_dist}{{2.6}{35}{Variational Inference for PMCs}{equation.2.3.6}{}}
\newlabel{eq:elbo_vpmc}{{2.7}{35}{Variational Inference for PMCs}{equation.2.3.7}{}}
\newlabel{eq:elbo_vpmc_l1}{{2.8}{35}{Variational Inference for PMCs}{equation.2.3.8}{}}
\newlabel{eq:elbo_vpmc_l2}{{2.9}{35}{Variational Inference for PMCs}{equation.2.3.9}{}}
\@writefile{brf}{\backcite{kingma2014}{{36}{2.3.2}{equation.2.3.9}}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces General parameter estimation for generative PMCs\relax }}{36}{algorithm.1}\protected@file@percent }
\newlabel{algo:algo_train_dpmc_gen}{{1}{36}{General parameter estimation for generative PMCs\relax }{algorithm.1}{}}
\newlabel{line:start_vpmc}{{2}{36}{General parameter estimation for generative PMCs\relax }{algorithm.1}{}}
\newlabel{line:evaluate_loss}{{6}{36}{General parameter estimation for generative PMCs\relax }{algorithm.1}{}}
\newlabel{line:derivate_pmc}{{7}{36}{General parameter estimation for generative PMCs\relax }{algorithm.1}{}}
\newlabel{eq:elbo_grad_vpmc}{{2.10}{36}{General parameter estimation for generative PMCs\relax }{equation.2.3.10}{}}
\newlabel{line:end_dtmc_vpmc}{{12}{36}{General parameter estimation for generative PMCs\relax }{equation.2.3.10}{}}
\newlabel{eq:reparametrization_vpmc}{{2.11}{36}{Variational Inference for PMCs}{equation.2.3.11}{}}
\newlabel{eq:elbo-pmc-mc-1}{{2.12}{36}{Variational Inference for PMCs}{equation.2.3.12}{}}
\citation{bayer2015learning,chung2015recurrent}
\citation{bayer2015learning,chung2015recurrent}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Experiments and results}{37}{section.2.4}\protected@file@percent }
\@writefile{brf}{\backcite{bayer2015learning}{{37}{2.4}{section.2.4}}}
\@writefile{brf}{\backcite{chung2015recurrent}{{37}{2.4}{section.2.4}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Model description}{37}{subsection.2.4.1}\protected@file@percent }
\@writefile{brf}{\backcite{bayer2015learning}{{37}{2.4.1}{subsection.2.4.1}}}
\@writefile{brf}{\backcite{chung2015recurrent}{{37}{2.4.1}{subsection.2.4.1}}}
\newlabel{eq:tmc-param}{{2.13}{37}{Model description}{equation.2.4.13}{}}
\citation{chung2015recurrent}
\newlabel{eq:pmc_qz}{{2.14}{38}{Model description}{equation.2.4.14}{}}
\@writefile{brf}{\backcite{chung2015recurrent}{{38}{2.4.1}{equation.2.4.14}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Results}{38}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Model configuration -}{38}{subsection.2.4.2}\protected@file@percent }
\citation{kingma2014adam}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Configuration of the dependencies for different deep generative \gls *{pmc}s. In each model, the sequence of latent variables $\{h_t\}_{t \in \mathbb  {N}}$ is treated as a deterministic variable given the observations. As a result, $\eta $ coincides with the Dirac measure. The distribution $\lambda $ is typically chosen to be Gaussian, while $\zeta $ depends on the nature of the observations. Remember that in a classical \gls *{rnn}, $\{z_t\}_{t \in \mathbb  {N}}$ is not considered.\relax }}{39}{table.caption.14}\protected@file@percent }
\newlabel{tab:config-pmc}{{2.1}{39}{Configuration of the dependencies for different deep generative \gls *{pmc}s. In each model, the sequence of latent variables $\{\Lat _t\}_{t \in \NN }$ is treated as a deterministic variable given the observations. As a result, $\eta $ coincides with the Dirac measure. The distribution $\lambda $ is typically chosen to be Gaussian, while $\zeta $ depends on the nature of the observations. Remember that in a classical \gls *{rnn}, $\{\Latent _t\}_{t \in \NN }$ is not considered.\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Training - }{39}{subsection.2.4.2}\protected@file@percent }
\@writefile{brf}{\backcite{kingma2014adam}{{39}{2.4.2}{subsection.2.4.2}}}
\@writefile{toc}{\contentsline {paragraph}{Evaluation - }{39}{subsection.2.4.2}\protected@file@percent }
\citation{lecun1998mnist}
\citation{bengio2013advances}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Dimensions of latent variables for each Deep PMC. $\psi ^{h}_{\theta }$, $\psi ^{z}_{\theta }$, $\psi ^{x}_{\theta }$ and $\psi ^{z}_{\phi }$ are implemented as neural networks with two hidden layers. The number of neurons on each layer coincide with $d_{h}$.\relax }}{40}{table.caption.15}\protected@file@percent }
\newlabel{tab:config}{{2.2}{40}{Dimensions of latent variables for each Deep PMC. $\ph $, $\pz $, $\px $ and $\qz $ are implemented as neural networks with two hidden layers. The number of neurons on each layer coincide with $d_{\lat }$.\relax }{table.caption.15}{}}
\@writefile{toc}{\contentsline {paragraph}{Image generation - }{40}{subsection.2.4.2}\protected@file@percent }
\@writefile{brf}{\backcite{lecun1998mnist}{{40}{2.4.2}{subsection.2.4.2}}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Averaged ELBO and approximated log-likelihood (approx. LL) of the observations on the test set with two different configurations. For the RNN, the ELBO coincides with the (exact) log-likelihood.\relax }}{41}{table.caption.16}\protected@file@percent }
\newlabel{tab:t1}{{2.3}{41}{Averaged ELBO and approximated log-likelihood (approx. LL) of the observations on the test set with two different configurations. For the RNN, the ELBO coincides with the (exact) log-likelihood.\relax }{table.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Examples of generated images from estimated $p_{\theta }(x_{0:t})$ for the MNIST data set with the PMC-II model.\relax }}{41}{figure.caption.17}\protected@file@percent }
\newlabel{fig:images}{{2.3}{41}{Examples of generated images from estimated $\p (\obs _{0:t})$ for the MNIST data set with the PMC-II model.\relax }{figure.caption.17}{}}
\@writefile{brf}{\backcite{bengio2013advances}{{42}{2.4.2}{figure.caption.17}}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Approximated likelihoods on the polyphonic music data sets. For the RNN, the exact log-likelihood is computed.\relax }}{42}{table.caption.18}\protected@file@percent }
\newlabel{tab:t2}{{2.4}{42}{Approximated likelihoods on the polyphonic music data sets. For the RNN, the exact log-likelihood is computed.\relax }{table.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Generative power of PMCs}{42}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1} Linear and stationary Gaussian PMCs }{43}{subsection.2.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear PMC -}{43}{subsection.2.5.1}\protected@file@percent }
\newlabel{eq:linear_pmc1}{{2.15}{43}{Linear PMC -}{equation.2.5.15}{}}
\newlabel{eq:linear_pmc2}{{2.16}{43}{Linear PMC -}{equation.2.5.16}{}}
\newlabel{eq:linear_pmc3}{{2.17}{43}{Linear PMC -}{equation.2.5.17}{}}
\@writefile{toc}{\contentsline {paragraph}{Gaussian PMC -}{43}{equation.2.5.17}\protected@file@percent }
\newlabel{eq:trans-var-pmc}{{2.18}{43}{Gaussian PMC -}{equation.2.5.18}{}}
\citation{salaun2019comparing}
\newlabel{eq:recursion_sigma}{{2.19}{44}{Gaussian PMC -}{equation.2.5.19}{}}
\@writefile{toc}{\contentsline {paragraph}{Stationary PMC - }{44}{equation.2.5.19}\protected@file@percent }
\newlabel{eq:constraints-pmc}{{2.20}{44}{Stationary PMC - }{equation.2.5.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Theoretical analysis of PMCs}{44}{subsection.2.5.2}\protected@file@percent }
\@writefile{brf}{\backcite{salaun2019comparing}{{44}{2.5.2}{subsection.2.5.2}}}
\newlabel{eq:covar-gum}{{2.21}{44}{Theoretical analysis of PMCs}{equation.2.5.21}{}}
\newlabel{prop:cov}{{2.5.1}{45}{}{proposition.2.5.1}{}}
\newlabel{eq:cov-pmc}{{2.22}{45}{}{equation.2.5.22}{}}
\citation{akhiezer1965classical}
\newlabel{prop:cov-pmc}{{2.5.2}{47}{}{proposition.2.5.2}{}}
\newlabel{eq:cov-pmm-e}{{2.23}{47}{}{equation.2.5.23}{}}
\newlabel{eq:cond-A-B-tilde}{{2.24}{47}{}{equation.2.5.24}{}}
\@writefile{brf}{\backcite{akhiezer1965classical}{{47}{2.5.2}{equation.2.5.24}}}
\citation{salaun2019comparing}
\@writefile{brf}{\backcite{salaun2019comparing}{{48}{2.5.2}{equation.2.5.24}}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Conclusions}{48}{section.2.6}\protected@file@percent }
\@setckpt{ch2}{
\setcounter{page}{49}
\setcounter{equation}{24}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{3}
\setcounter{table}{4}
\setcounter{NAT@ctr}{0}
\setcounter{nlinenum}{0}
\setcounter{etoc@tocid}{3}
\setcounter{blindtext}{1}
\setcounter{Blindtext}{5}
\setcounter{blind@countparstart}{0}
\setcounter{blindlist}{0}
\setcounter{blindlistlevel}{0}
\setcounter{blindlist@level}{0}
\setcounter{blind@listcount}{0}
\setcounter{blind@levelcount}{0}
\setcounter{blind@randomcount}{0}
\setcounter{blind@randommax}{0}
\setcounter{blind@pangramcount}{0}
\setcounter{blind@pangrammax}{0}
\setcounter{section@level}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{31}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{tcbbreakpart}{0}
\setcounter{tcblayer}{0}
\setcounter{tcolorbox@number}{0}
\setcounter{FN@totalid}{1}
\setcounter{pp@a@FN@totalid}{1}
\setcounter{lstnumber}{1}
\setcounter{parentequation}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{AM@survey}{0}
\setcounter{float@type}{32}
\setcounter{algorithm}{1}
\setcounter{ALG@line}{12}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{definition}{0}
\setcounter{example}{0}
\setcounter{remark}{0}
\setcounter{remarks}{0}
\setcounter{proposition}{0}
\setcounter{theorem}{0}
\setcounter{usecaseinner}{0}
\setcounter{lemma}{0}
\setcounter{model}{0}
\setcounter{corollary}{0}
\setcounter{scenario}{0}
\setcounter{su@anzahl}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{lstlisting}{0}
}
